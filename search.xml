<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[常用的降维技术和主题模型简介]]></title>
      <url>%2F2017%2F04%2F19%2FML5-Topic-Model%2F</url>
      <content type="text"><![CDATA[本文主要介绍了 LDA 主题模型及其在 R 语言中的实现方法 在机器学习中，LDA 是两个常用模型的简称：Linear Discriminant Analysis 和 Latent Dirichlet Allocation，这篇文章主要介绍后者。LDA (Latent Dirichlet Allocation) 是一种很常用的主题模型，类似于 LSA 和 PLSA 等模型，可以用于浅层语义分析，为了避免混淆，首先说明这些概念的区别。 Dimension Reduction Techniques降维技术是很多主题模型的基础，常用的降维技术包括：PCA、FA、SVD 和 Linear Discriminant Analysis。其中 PCA、FA 和 SVD 主要关注未标记数据上的降维技术，但也可同时应用于已标记的数据，而 Linear Discriminant Analysis 则是针对已标记数据的降维技术 Principle Component AnalysisPCA（Principle Component Analysis）：主成分分析是一种降维技术，将过多的变量综合为少数几个概括性的新变量对原始目标进行解释。在主成分分析中，数据从原来的坐标系转换到了新的坐标系，新坐标系的选择由数据本身决定。第一个坐标轴选择原始数据中方差最大的方向，第二个坐标轴选择与第一个坐标轴正交且有最大方差的方向。重复此过程，大部分方差都会包含在前面的几个新坐标轴中，因此可以忽略余下的坐标轴，即对数据机进行了降维处理。 Factor AnalysisFA（Factor Analysis）：在因子分析中，假设在观察数据的生成过程中有一些观察不到的隐变量，假设观察数据是这些隐变量和某些噪声的线性组合，从而可以通过找到这些隐变量来实现数据的降维。 Singular Value DecompositionSVD（Singular Value Decomposition）：奇异值分解是一种适用于任何矩阵的分解方法。假设原始矩阵 $A$ 是一个 $M \times N$ 的矩阵，那么可以将 $A$ 分解为： $$A{m \times n}=U{m \times m} \Sigma{m \times n} V{n \times n}^T$$ 上述分解会构造出一个矩阵 $\Sigma$，该矩阵为对角阵，除了对角元素，其他元素均为 0，惯例是 $\Sigma$ 中的元素从大到小排列，这些对角元素称为奇异值（Sigular Value） 和 PCA 中得到矩阵的特征值类似，奇异值也是告诉我们数据中的重要特征，奇异值与特征值的关系是：奇异值就是矩阵 $A \times A^T$ 特征值的平方根 如果矩阵 $\Sigma$ 的对角元素按从大到小排列，那么在某个数目（r 个）的奇异值之后，其他的奇异值都置为 0，这就意味着数据集中仅有 r 个重要特征，其余特征均是噪声或冗余特征 选取奇异值的数据，可以先计算奇异值的平方，然后将奇异值平方和累加到总值 90% 的数目作为最终选取的重要特征数 在推荐系统中的应用：简单的推荐系统能够计算 item 或 user 之间的相似度，从而进行基于内容的推荐或协同过滤推荐。进一步提升推荐效果的办法则是先利用 SVD 从数据（往往是一个非常稀疏的矩阵）中构建一个主题空间，然后再在该空间下计算其相似度。 Linear Discriminant Analysis线性判别分析（Linear Discriminant Analysis），简称为 LDA，也被称为 Fisher 线性判别，是模式识别的经典算法。其基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息（分类）和压缩特征空间维数（降维）的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。 LDA 与前面介绍过的 PCA 都是常用的降维技术。PCA 主要是从特征的协方差角度，去找到比较好的投影方式。LDA 更多的是考虑了标注，即希望投影后不同类别之间数据点的距离更大，同一类别的数据点更紧凑。 PCA 降维是直接和数据维度相关的，比如原始数据是 n 维的，那么 PCA 后，可以任意选取 1 维、2 维，一直到 n 维都行（当然是对应特征值大的那些）。LDA 降维是直接和类别的个数相关的，与数据本身的维度没关系，比如原始数据是 n 维的，一共有 C 个类别，那么 LDA 降维之后，一般就是 1 维，2 维到 C-1 维进行选择（当然对应的特征值也是最大的一些） PCA 投影的坐标系都是正交的，而 LDA 根据类别的标注，关注分类能力，因此不保证投影到的坐标系是正交的（一般都不正交） Basic Topic Models在文本挖掘中，通常有很多文本文档，比如博客或新闻等，而我们的目的是将这些文本分成我们能够理解的不同类别。主题模型是一种无监督的文档分类方法，和对数值型数据进行聚类类似，主题模型是对文档进行“聚类”。 常用的主题模型包括 LSA（Latent Semantic Analysis）、PLSA（Probabilistic Latent Semantic Analysis） 和 LDA（Latent Dirichlet Allocation），但不管何种算法，主题模型的本质都是要计算两个矩阵 Word 对 Topic 的矩阵 Topic 对 Document 的矩阵 LSALSA 采用暴力 SVD 矩阵分解，如果维数太大，计算效率会很低。在 LSA 中，一个矩阵由文档和单词组成，当我们在矩阵上应用 SVD 时，就会构建出多个奇异值，每个奇异值代表文档中的一个概念或主题。 LSA 的具体实例参见 这篇博客 除了 LSA 之外，下面我们将介绍几个和 LDA 相关的基础模型：Unigram model、mixture of unigrams model、pLSA model。为了方便描述，首先定义一些变量： $w$ 表示词，$V$ 表示所有词的个数 $z$ 表示主题，$k$ 表示主题的个数 $D=(W_1,W_2,…,W_M)$表示语料库，M表示语料库中的文档数。 $W=(w_1,w_2,…,w_N)$表示文档，N表示文档中词的个数。 Unigram Model在一元模型中，对于文档 $W=(w_1,w_2,…,w_N)$ ，用 $p(w_n)$ 表示 $w_n$ 的先验概率，生成文档 $W$ 的概率为： $$P(W)=\prod_{n=1}^n p(w_n)$$ 其图模型为（图中被涂色的 w 表示可观测变量，N 表示一篇文档中总共 N 个单词，M 表示 M 篇文档） Mixture of Unigrams Model混合一元模型的生成过程是：给某个文档先选择一个主题 Z，再根据该主题生成文档，该文档中的所有词都来自一个主题。生成文档的概率 其图模型为（图中被涂色的 w 表示可观测变量，未被涂色的 z 表示未知的隐变量，N 表示一篇文档中总共 N 个单词，M 表示 M 篇文档） PLSA在混合一元模型中，假定一篇文档只由一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。在 PLSA 中，假设文档由多个主题生成。下面通过一个例子说明 PLSA 生成文档的过程 加入我们有三个主题，每个主题下有三个可选词，在生成文档的过程中，每写一个词，先扔“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。 上面这个投骰子产生词的过程简化一下便是：“先以一定的概率选取主题，再以一定的概率选取词”。事实上，一开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如可能选取教育主题的概率是 0.5，选取经济主题的概率是 0.3，选取交通主题的概率是 0.2，那么这 3 个主题的概率分布便是{教育：0.5，经济：0.3，交通：0.2}，我们把各个主题 z 在文档 d 中出现的概率分布称之为主题分布，且是一个多项分布。 同样的，从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程，这3个词都可能被选中，但它们被选中的概率也是不一样的。比如大学这个词被选中的概率是0.5，老师这个词被选中的概率是0.3，课程被选中的概率是0.2，那么这3个词的概率分布便是{大学：0.5，老师：0.3，课程：0.2}，我们把各个词语 w 在主题 z 下出现的概率分布称之为词分布，这个词分布也是一个多项分布。 最后，不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复 N 次（产生 N 个词），完成一篇文档，重复这产生一篇文档的方法 M 次，则完成 M 篇文档 利用看到的文档推断其隐藏的主题（分布）的过程，就是主题建模的目的：自动地发现文档集中的主题（分布）。文档 d 和单词 w 是可被观察到的，但主题 z 却是隐藏的。如下图所示（图中被涂色的 d、w 表示可观测变量，未被涂色的 z 表示未知的隐变量，N 表示一篇文档中总共 N 个单词，M 表示 M 篇文档）。 对于任意一篇文档，$P(w_j|d_i)$ 是已知的，根据这个概率可以训练得到“文档-主题”概率以及“主题-词项”概率，即： 每篇文档中每个词的生成概率为： $P(d_i)$ 可以直接得出，而 $P(z_k|d_i)$ 和 $P(w_j|z_k)$ 未知，所以 $\theta=(P(z_k|d_i),P(w_j|z_k))$就是我们要估计的参数，我们要最大化这个参数。因为该待估计的参数中含有隐变量 z，所以我们可以用 EM 算法来估计这个参数。 PLSA 把 LSA 变成从概率的角度理解，把 LSA 的 2 个矩阵做归一化处理后，就可以看成 PLSA 的 Word 对于 Topic 的概率分布和 Document 对于 Topic 的概率分布。然后采用 EM 算法，先随机初始化这 2 个分布，计算其后验概率，然后反过来利用最大似然来计算这 2 个分布，不断迭代直到收敛，但 PLSA 已经基本上被 LDA 所取代。 LDA Topic ModelLDA（Latent Dirichlet Allocation, 潜狄利克雷分布）是一种概率主题模型，它在 PLSA 的基础上，利用贝叶斯估计，引入 Topic 的 Dirichlet 先验分布后得到贝叶斯模型，相当于多了一个条件，而后采用 Gibbs sampling 的方式来学习参数，收敛后计算得到 Word 对于 Topic 的概率分布和 Topic 对于 Document 的概率分布。 在 PLSA 中，样本随机，参数虽未知但固定，所以 PLSA 属于频率派思想。区别于下文要介绍的LDA中，样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以 LDA 属于贝叶斯派思想。 LDA 涉及的数学知识很多，包括 Gamma 函数、Dirichlet 分布、Dirichlet-Multinomial 共轭、Gibbs Sampling、Variational Inference、PLSA 等，暂且不论其背后的数学逻辑，理解 LDA 模型最重要的两点是 每个文档由多个主题构成，例如 “Document 1 由 90% 的 Topic A 和 10% 的 Topic B 构成” 每个主题由多个单词构成，例如 “政治主题最常见的单词是 President、Congress 和 Government” LDA是一种典型的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。 Mathematical BasisBeta-Binomial Conjugate Beta 分布可以看做是概率的概率分布，当你不确定一个事件的具体概率是多少时，它可以给出了所有概率出现的可能性大小。Beta 分布的概率密度函数如下，B 函数是一个标准化函数，它只是为了使得这个分布的概率密度积分等于 1 才加上的。 , where 二项分布是由伯努利分布推出的。伯努利分布，又称两点分布或 0-1 分布，是一个离散型的随机分布，其中的随机变量只有两类取值，即 0 或 1。二项分布是重复 n 次的伯努利试验。简言之，只做一次实验，是伯努利分布，重复做了 n 次，是二项分布。二项分布的概率密度函数为 $$P(K=k)=C(n,k)p^k(1-p)^{n-k}, where \ C(n,k)=\frac{n!}{k!(n-k)!}$$ 轭的意思是束缚、控制。共轭（conjugate）从字面上理解，则是共同约束，或互相约束。在贝叶斯概率理论中，如果后验概率 $P(z|x)$ 和先验概率 $p(z)$ 满足同样的分布，那么，先验分布和后验分布被称为共轭分布，同时，先验分布叫做似然函数的共轭先验分布。 二项分布的似然函数为 $P(data|\theta)\propto \theta^k (1-\theta)^{n-k}$ Beta 分布为：$Beta(\alpha,\beta)=\theta^{\alpha-1}(1-\theta)^{\beta-1}/B(\alpha,\beta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$ 贝叶斯估计的目的就是要在给定数据的情况下求出参数 θ 的值，所以我们的目的是求解后验概率，因为 data 与待估参数独立，所以不予考虑，我们称 $P(data|\theta)$ 为似然函数，$P(\theta)$ 为先验分布 $$P(\theta|data)=\frac{P(data|\theta)P(\theta)}{P(data)}\propto P(data|\theta)P(\theta)$$ 现在我们将服从 Beta 分布的先验分布带入可以得到$$P(\theta|data)=\theta^k (1-\theta)^{n-k}\ * \ \theta^{\alpha-1}(1-\theta)^{\beta-1}\ \propto \theta^{\alpha+k-1}(1-\theta)^{\beta+n-k-1}$$ 因此，贝叶斯估计得到的后验概率仍然服从 Beta 分布，所以说 Beta 分布是二项分布的共轭先验分布！ 下面举一个简单的例子来说明 Beta 分布和二项分布是共轭分布，熟悉棒球运动的都知道有一个指标就是棒球击球率 (batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为 0.266 是正常水平的击球率，而如果击球率高达 0.3 就被认为是非常优秀的。现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是 100% 了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是 0.215 到 0.36 之间才算正常。 对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），表示这些经验最好的方法（在统计中称为先验信息）就是用 beta 分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta 分布的定义域是(0,1)，这跟概率的范围是一样的。 接下来我们将这些先验信息转换为 beta 分布的参数，我们知道一个击球率应该是平均 0.27 左右，而他的范围是 0.21 到 0.35，那么根据这个信息，我们可以取 $\alpha=81、\beta=219$。那么，$Beta(\alpha,\beta)$ 分布的均值为：$mean=\frac{\alpha}{\alpha+\beta}=\frac{81}{81+219}=0.27$。 如下图中的红线所示，x 轴就表示各个击球率的取值，x 对应的 y 值就是这个击球率所对应的概率。也就是说 beta 分布可以看作一个概率的概率分布。那么有了先验信息后，现在我们考虑一个运动员只打一次球，那么他现在的数据就是“1击1中”，即 hit=1/miss=0。这时候我们就可以更新我们的分布了，让这个曲线做一些移动去适应我们的新信息。beta 分布在数学上就给我们提供了这一性质，他与二项分布是共轭先验的（Conjugate_prior）。所谓共轭先验就是先验分布是 $Beta(\alpha,\beta)$ 分布，而后验分布同样是 beta 分布： $Beta(\alpha+hit,\beta+miss)=Beta(81+1,219+0)$。 如下图中的绿线所示，可以看出更新之后的 PDF 变化不大，因为只打一次球并不能说明什么问题。但如果我们有更多的数据，比如一共打了 300 次，击中 100 次，未击中 200 次，再次更新之后 PDF 变为： $Beta(81+100,219+200)$。 如下图中的蓝线所示，可以看到更新后的曲线变得更尖，并且向右平移了一段距离，说明比平均水平要高。新分布的数学期望为 $\frac{181}{181+419}=0.302$，这比直接估计的结果 $\frac{100}{100+200}=0.333$ 小，这是因为我们包含了先验信息，可以理解为在这个运动员在击球之前他已经成功了 81 次，失败了 219 次。 Dirichlet-Multinomial Conjugate Dirichlet 分布式 Beta 分布在高维度上的推广，两者的 PDF 类似： where 多项分布（Multinomial Distribution）是二项分布在高纬度上的推广，多项分布是指单次试验中的随机变量的取值不再是0-1，而是有多种离散值可能（1,2,3…,k）。比如投掷 6 个面的骰子，N 次实验结果服从 K=6 的多项分布。 Dirichlet 和 Multinomial 共轭分布的证明较为复杂，暂不深究，但道理和 Beta-Binomial 共轭类似。至此，我们可以看到二项分布和多项分布很相似，Beta 分布和 Dirichlet 分布很相似。并且 Beta 分布是二项式分布的共轭先验概率分布。同样道理，Dirichlet 分布是多项式分布的共轭先验概率分布。 Document Generation Process当我们看到一篇文章后，往往会推测这篇文章是如何生成的，我们通常认为作者会先确定几个主题，然后围绕这几个主题遣词造句写成全文。LDA 要干的事情就是根据给定的文档，判断它的主题分布，以及每个主题下的单词分布。如下图所示，在 LDA 模型中，生成文档的过程有以下几步： 从狄利克雷分布 $\alpha$ 中生成文档 $i$ 的主题分布 $\theta_i$ 从主题的多项式分布 $\thetai$ 中取样生成文档 $i$ 第 $j$ 个词的主题 $Z{i,j}$ 从狄利克雷分布 $\beta$ 中取样生成主题 $Z{i,j}$ 对应的词语分布 $\Phi{i,j}$ 从词语的多项式分布 $\Phi{i,j}$ 中采样最终生成词语 $W{i,j}$ 在 LDA 网络的贝叶斯结构中，阴影圆圈表示可观测的变量，非阴影圆圈表示隐变量，箭头表示两变量间的条件依赖性 （conditional dependency），方框表示重复抽样，方框右下角的数字代表重复抽样的次数。$\varphi$ (Topic-Word 分布) 和 $\theta$ (Doc-Topic 分布) 是 Dirichlet distributions， $\theta$ 指向 $Z$ 是从 Doc-Topic 分布中采样一个主题赋给 $W$，$\varphi$ 指向 $W$ 是 $\varphi$ 的 Topic-Word 分布依赖于 $W$。 $K$ 表示主题数，$N$ 表示各文档中的单词数，$M$ 表示文档数 $\alpha$ 是 Document-Topic 的 Dirichlet-prior Distribution $\beta$ 是 Topic-Word 的 Dirichlet-prior Distribution $\theta_i$ 是文档 $i$ 的主题分布 $\varphi_k$ 是主题 $k$ 的单词分布 $W_{i,j}$ 是文档 $i$ 的单词 $j$ $Z{i,j}$ 是 $W{i,j}$ 对应的主题 拿之前讲解 PLSA 的例子进行具体说明。如前所述，在 PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。举个文档d产生主题z的例子。在 PLSA 中，给定一篇文档 d，主题分布是一定的，比如 {P(zi|d), i = 1,2,3}可能就是 {0.4,0.5,0.1}，表示 z1、z2、z3 这3个主题被文档 d 选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示: 而在LDA中，选主题和选词依然都是两个随机的过程，但是主题分布和词的分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不能确定，因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，主题分布和词分布由 Dirichlet 先验确定。 换言之，LDA 在 PLSA 的基础上给 $P(z_k|d_i)$ 和 $P(w_j|z_k)$ 这两个参数加了两个先验分布（贝叶斯化）。 综上，LDA 真的只是 PLSA 的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在 PLSA 中用极大似然估计的思想去推断两未知的固定参数，而 LDA 则把这两参数弄成随机变量，且加入 dirichlet 先验。所以，PLSA跟 LDA 的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是频率派思想，后者用的是贝叶斯派思想。 比如，我去一朋友家： 按照频率派的思想，我估计他在家的概率是 1/2，不在家的概率也是 1/2，是个定值 而按照贝叶斯派的思想，他在家不在家的概率不再认为是个定值 1/2，而是随机变量。比如按照我们的经验（比如当天周末），猜测他在家的概率是 0.6，但这个 0.6 不是说就是完全确定的，也有可能是 0.7。如此，贝叶斯派没法确切给出参数的确定值，但至少明白在哪个范围或哪些取值（0.6、0.7、0.8、0.9）更有可能，哪个范围或哪些取值（0.3、0.4） 不太可能。进一步，贝叶斯估计中，参数的多个估计值服从一定的先验分布，而后根据实践获得的数据（例如周末不断跑他家），不断修正之前的参数估计，从先验分布慢慢过渡到后验分布 LDA 生成文档的过程中，先从 dirichlet 先验中“随机”抽取出主题分布，然后从主题分布中“随机”抽取出主题，选取不同的参数 $\alpha$，dirichlet 分布会偏向不同的主题。Document-Topic 和 Topic-Word 的 Dirichlet-Multinomial 共轭结构如下图： Parameter Estimation 在 PLSA 中，我们使用 EM 算法去估计“主题-词项”矩阵和“文档-主题”矩阵这两个参数，而且这两参数都是个固定的值，只是未知，使用的思想其实就是极大似然估计 MLE。 而在 LDA 中，估计这两未知参数可以用变分 (Variational inference)-EM算法，也可以用 gibbs 采样，前者的思想是最大后验估计MAP（MAP与MLE类似，都把未知参数当作固定的值），后者的思想是贝叶斯估计。贝叶斯估计是对 MAP 的扩展，但它与 MAP 有着本质的不同，即贝叶斯估计把待估计的参数看作是服从某种先验分布的随机变量. LDA 的原始论文中是用的“变分-EM”算法估计未知参数，后来发现另一种估计 LDA 未知参数的方法更好，这种方法就是：Gibbs Sampling。Gibbs 采样是马尔可夫链蒙特卡尔理论（MCMC）中用来获取一系列近似等于指定多维概率分布（比如 2 个或者多个随机变量的联合概率分布）观察样本的算法，Gibbs Sampling 的大致流程如下： 基于上述 LDA 模型的文档生成过程，整个模型所有可见变量以及隐藏变量的联合分布是 最终一篇文档的单词分布的最大似然估计可以通过将上式的 $\theta_i$ 以及 $\Phi$ 进行积分和对 $z_i$ 进行求和得到。然后，根据 $p(w_i|\alpha ,\beta )$ 的最大似然估计，最终可以通过吉布斯采样等方法估计出模型中的参数。 因为 $\alpha$ 产生主题分布 $\theta$，主题分布 $\theta$ 确定具体主题，且 $\beta$ 产生词分布 $\varphi$、词分布 $\varphi$ 确定具体词，所以所有变量的联合概率分布其实等价于： $$p(w,z|\alpha,\beta)=p(z|\alpha)p(w|z,\beta)$$ 由于此公式第一部分独立于 $\beta$，第二部分独立于 $\alpha$，所以可分别处理。根据贝叶斯法则和 Dirichlet 先验，以及上文中得到的 $p(z|\alpha)$ 和 $p(w|z,\beta)$ 各自被分解成两部分乘积的结果可以计算得到每个文档上 Topic 的后验分布和每个 Topic 下的词的后验分布（据上文可知：其后验分布跟它们的先验分布一样，也都是 Dirichlet 分布），最终求解 Topic-Word 和 Doc-Topic 的 Dirichlet 分布的期望为： 具体的推导过程参见 这篇博客 这样就计算出了最终我们要求的分布参数（注意分布参数的计算要在 sampling 收敛阶段进行）。有了 Dirichlet 后验分布的期望，排除当前词的主题分配，即根据其他词的主题分配和观察到的单词来计算当前词主题的概率公式为 这个式子的右半部分便是 $p(Topic|Doc)*p(Word|Topic)$，这个概率的值对应 $Doc\rightarrow Topic \rightarrow Word$ 的路径概率。如此，K 个 Topic 对应着 K 条路径，Gibbs Sampling 便在这 K 条路径中进行采样，如下图所示: Gibbs Sampling in LDA第一阶段：初始化 输入单词向量 $w$，超参数 $\alpha$ 和 $\beta$，主题数 $K$ 全局变量：$n_m^{(k)}$ 文档 m 中 主题 k 出现的次数；$n_m$ 文档 m 中的主题总数；$n_k^{(t)}$ 主题 k 中单词 t 出现的次数；$n_k$ 主题 k 中的单词总数。将这四个全局变量全部初始化为 0。 对所有文档 $m\in [1,M]$： 对文档 $m$ 中的所有单词 $n$： 采样每个单词对应的主题 $z_{m,n}=k~Mult(1/K)$ 增加“文档-主题”计数：$n_m^{(k)}+=1$ 增加“文档-主题”总数：$n_m+=1$ 增加“主题-词项”计数：$n_k^{(t)}+=1$ 增加“主题-词项”总数：$n_k+=1$ 第二阶段：burn-in（Gibbs 未收敛阶段） 对所有文档 $m\in [1,M]$： 对文档 $m$ 中的所有单词 $n$： 假如当前文档 $m$ 的词 $t$ 对应主题为 $k$，减少计数 $n_m^{(k)}-=1$，$n_m-=1$，$n_k^{(t)}-=1$，$n_k-=1$，即先拿出当前词 之后根据 LDA 中 Topic sample 的概率分布 sample 出新的主题，然后对应的四个全局变量再分别 +1 第三阶段：sampling（Gibbs 收敛阶段） 迭代完成后，输出 Topic-Word 参数矩阵 $\phi$ 和 Doc-Topic 矩阵 $\theta$ Latent Dirichlet Allocation in RDocument-Term Matrix文本挖掘最常见的数据结构 Document-Term Matrix (or DTM)，通常情况下： 行代表 Document（比如一本书或一篇文章） 列代表 Term 矩阵的值代表 Term 在 Document 中出现的次数 但一般来说，DTM 都是一个十分稀疏的矩阵，因此在 R 中会有专门的数据结构对其进行更有效率的存储。 DocumentTermMatrix (tm)R 中最常用的 DTM 数据结构是 tm 包中的 DocumentTermMatrix，很多文本挖掘的数据集都以这种形式提供，例如美联社新闻数据集，包括 2246 篇文档和 10473 个单词，稀疏度 99% 表示 DTM 中 99% 的值为 0。 123456789library(Topicmodels)data("AssociatedPress")AssociatedPress## &lt;&lt;DocumentTermMatrix (Documents: 2246, terms: 10473)&gt;&gt;## Non-/sparse entries: 302031/23220327## Sparsity : 99%## Maximal term length: 18## Weighting : term frequency (tf) tbl_df (dplyr)如果我们想用 dplyr、tidytext、ggplot2 等包对文档进行分析，则需要将 DTM 转换为 one-token-per-Document-per-row 的形式，该形式只保留非零值。 12345678910111213141516171819library(dplyr)library(tidytext)ap_td &lt;- tidy(AssociatedPress)ap_td## # A tibble: 302,031 × 3## Document term count## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;## 1 1 adding 1## 2 1 adult 2## 3 1 ago 1## 4 1 alcohol 1## 5 1 allegedly 1## 6 1 allen 1## 7 1 apparently 2## 8 1 appeared 1## 9 1 arrested 1## 10 1 assault 1## # ... with 302,021 more rows 然后，我们就可以对其进行包括情感分析在内的诸多分析。12345678910111213141516171819202122232425262728293031ap_sentiments &lt;- ap_td %&gt;% inner_join(get_sentiments("bing"), by = c(term = "Word"))ap_sentiments## # A tibble: 30,094 × 4## Document term count sentiment## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;## 1 1 assault 1 negative## 2 1 complex 1 negative## 3 1 death 1 negative## 4 1 died 1 negative## 5 1 good 2 positive## 6 1 illness 1 negative## 7 1 killed 2 negative## 8 1 like 2 positive## 9 1 liked 1 positive## 10 1 miracle 1 positive## # ... with 30,084 more rows# 最长用的正面词和负面词library(ggplot2)ap_sentiments %&gt;% count(sentiment, term, wt = count) %&gt;% ungroup() %&gt;% filter(n &gt;= 200) %&gt;% mutate(n = ifelse(sentiment == "negative", -n, n)) %&gt;% mutate(term = reorder(term, n)) %&gt;% ggplot(aes(term, n, fill = sentiment)) + geom_bar(stat = "identity") + ylab("Contribution to sentiment") + coord_flip() LDA Code in R用 R 中 Topicmodels 包中的 LDA 函数实现主题模型，以主题数 k=2 为例1234# set a seed so that the output of the model is predictableap_lda &lt;- LDA(AssociatedPress, k = 2, control = list(seed = 1234))ap_lda## A LDA_VEM Topic model with 2 Topics. 然后，用 tidytext 包中的 tidy() 方法可以从 LDA() 的结果中提取 Word-Topic probabilities 和 Document-Topic probabilities Word-Topic Probabilities123456789101112131415161718library(tidytext)ap_Topics &lt;- tidy(ap_lda, matrix = "beta")ap_Topics## # A tibble: 20,946 × 3## Topic term beta## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;## 1 1 aaron 1.686917e-12## 2 2 aaron 3.895941e-05## 3 1 abandon 2.654910e-05## 4 2 abandon 3.990786e-05## 5 1 abandoned 1.390663e-04## 6 2 abandoned 5.876946e-05## 7 1 abandoning 2.454843e-33## 8 2 abandoning 2.337565e-05## 9 1 abbott 2.130484e-06## 10 2 abbott 2.968045e-05## # ... with 20,936 more rows 从中，我们可以看出每个 Word 由各个 Topic 产生的概率，比如 “arron” 有 1.686917e-12 的概率由 Topic 1 产生，有 3.895941e-05 的概率由 Topic 2 产生。除此之外，可以进一步统计每个 Topic 中出现最多的词，以及在两个 Topic 中差异最大的词 Document-Topic probabilities1234567891011121314151617ap_Documents &lt;- tidy(ap_lda, matrix = "gamma")ap_Documents## # A tibble: 4,492 × 3## Document Topic gamma## &lt;int&gt; &lt;int&gt; &lt;dbl&gt;## 1 1 1 0.2480616686## 2 2 1 0.3615485445## 3 3 1 0.5265844180## 4 4 1 0.3566530023## 5 5 1 0.1812766762## 6 6 1 0.0005883388## 7 7 1 0.7734215655## 8 8 1 0.0044516994## 9 9 1 0.9669915139## 10 10 1 0.1468904793## # ... with 4,482 more rows 从 Document-Topic 概率中，我们可以看出每个文档的词由各个主题产生的比例。比如，Document 1 中只有 24.8% 的词是由 Topic 1 产生的，而 Document 6 中几乎所有的词都由 Topic 2 产生（可以通过查看 Document 6 中的词验证其确实属于政治主题）。 123456789101112131415161718tidy(AssociatedPress) %&gt;% filter(Document == 6) %&gt;% arrange(desc(count))## # A tibble: 287 × 3## Document term count## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;## 1 6 noriega 16## 2 6 panama 12## 3 6 jackson 6## 4 6 powell 6## 5 6 administration 5## 6 6 economic 5## 7 6 general 5## 8 6 i 5## 9 6 panamanian 5## 10 6 american 4## # ... with 277 more rows 更多文本挖掘和 LDA 可视化的详细介绍参见 Reference：Topic Modeling in R Reference Latent Dirichlet Allocation Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. Latent Dirichlet Allocation from Github.com 主题模型TopicModel：隐含狄利克雷分布LDA - CSDN.NET. (2017). Blog.csdn.net. Retrieved 21 April 2017 隐含狄利克雷分布 | Wikiwand. (2017). Wikiwand. Retrieved 21 April 2017 通俗理解LDA主题模型 - CSDN.NET. (2017). Blog.csdn.net. Retrieved 21 April 2017 LDA 数学八卦, Rickjin, Version 1 Distribution Beta distribution | Wikiwand. (2017). Wikiwand. Retrieved 21 April 2017 (2017). 如何通俗地理解 Beta 分布. Zhihu.com. Retrieved 21 April 2017 Dirichlet distribution | Wikiwand. (2017). Wikiwand. Retrieved 21 April 2017 Multinomial distribution | Wikiwand. (2017). Wikiwand. Retrieved 21 April 2017 Topic Modeling in R Robinson, J. (2017). Text Mining with R. Tidytextmining.com. Retrieved 14 April 2017 David Meza. (2017). Topic Modeling in R. Retrieved 19 April 2017 Others 《机器学习实战》 哈林顿 (Peter Harrington), 李锐, 李鹏, 曲亚东, 王斌. (2017). Amazon.cn. Retrieved 14 April 2017 LDA 线性判别分析 - porly的专栏 - 博客频道 - CSDN.NET. (2017). Blog.csdn.net. Retrieved 14 April 2017 (2017). LDA 与 LSA、PLSA、NMF相比，哪个效果更好？为什么？Zhihu.com. Retrieved 14 April 2017 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园. (2017). Cnblogs.com. Retrieved 14 April 2017 Latent Semantic Analysis (LSA) Tutorial. (2011). Personal Wiki. Retrieved 14 April 2017]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[模型评估与选择概述]]></title>
      <url>%2F2017%2F04%2F12%2FML4-Prediction-Accuracy-2%2F</url>
      <content type="text"><![CDATA[本文总结讨论了模型（主要针对二分类问题）评估与选择常用方法 Hold-out Method留出法直接将数据集 D 分为两个互斥的集合，其中一个作为训练集（Training Set） S，另一个作为测试集(Testing Set) T。在 S 上训练模型后，用 T 来评估其预测误差，作为对泛化误差的估计 训练集和测试集的划分要尽量保持数据分布的一致性，比如采用分层采样 单次使用留出法得到的结果往往不够稳定可靠，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果，同时可以得到估计结果的标准差 常用的做法是将大约 2/3~4/5 的样本用于训练，剩余样本用于测试 Cross Validation交叉验证法先将数据集划分为 k 个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。然后，每次用 k-1 个子集进行训练，余下的那个子集作为测试集，从而可以进行 k 次训练和测试，最终返回这 k 个测试结果的均值 交叉验证结果的稳定性很大程度上取决于 k 的取值，k 通常取值 10、5、20 k 折交叉验证通常要随机使用不同的划分重复 p 次，最终评价结果是这 p 次 k 折交叉验证结果的均值，例如“10次10折交叉验证” 假定数据集 D 中有 m 个样本，若 k=m，则得到交叉验证的一个特例：“留一法”，留一法不受样本随机划分的影响，但是计算开销较大。 Parameter Tuning机器学习中通常涉及两类参数： 算法的参数（“超参数”）：数目常在 10 以内，通常由人工设定多个参数候选值后产生模型 模型的参数：数目可能很多，例如深度学习模型甚至有上百亿个参数，一般通过学习来产生多个候选模型（例如神经网络在不同轮数停止训练） Grid Search：对每个参数选定一个范围和变化步长，所有参数的组合数目即为所需训练的模型数，这是在计算开销和性能估计之间进行折中的结果。 我们通常把学得模型在实际使用中遇到的数据称为 Testing Set，而模型评估与选择中用于评估测试的数据集通常被称为 Validation Set。例如，用测试集上的判别效果来估计模型在实际使用时的泛化性能，而把训练集数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参 Precision / Recall / F1以二分类问题为例，可以将样例根据其真实类别与分类算法预测类别的组合划分为： TP: true positive FP: false positive TN: true negative FN: false negetive Confusion Matrix分类结果的“混淆矩阵”（Confusion Matrix）如下： Precision &amp; Recall根据混淆矩阵定义 Accuracy, Precision, Recall 如下： $$Accuracy=\frac{TP+TN}{TP+FP+TN+FN}$$$$Precision=\frac{TP}{TP+FP}$$$$Recall=\frac{TP}{TP+FN}$$ P-R Curve查准率（Precision，也叫“准确率”）和查全率（Recall，也叫“召回率”）是一对矛盾的度量，通常在一些简单的任务中，才可能同时使查准率和查全率都很高。 在很多情形下，我们可以根据预测结果对样例进行排序，排在最前面的是有可能是正例的样本，排在最后的是最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次均可计算出一对 Precision 和 Recall，分别以 Precision 和 Recall 为纵轴和横轴，就可以得到“P-R 曲线” 如果一个学习器的 P-R 曲线被另一个学习器的 P-R 曲线完全包住（比如 C 被 B 完全包住），则可以判定后者的性能优于前者 如果两个学习器的 P-R 曲线发生交叉（比如 A 与 B 发生交叉），则难以断言二者孰优孰劣，只能在具体的情况下进行比较 然而在很多情况下，如果仍然希望将学习器 A 和 B 比个高低，这是一个比较合理的判据是 P-R 曲线下面积的大小 (AUC-PR)，但这个面积不太容易估算，所以就需要一个综合 Precision 和 Recall 的指标 F1 Score平衡点（Break-Even Point）是 Precision = Recall 时的取值，但 BEP 还是过于简化了些，更常用的是 F1 度量 $$F1=\frac{2 \times Precision \times Recall}{Precision+Recall}=\frac{2 \times TP}{样例总数+TP-TN}$$ 在不同的应用中，对 Precision 和 Recall 的重视程度可能有所不同。例如在商品推荐中，为了尽可能减少对用户的打扰，Precision 更重要；而在逃犯检索系统中，为了尽可能不漏掉逃犯，Recall 更重要 实际上，F1 是基于 Precision 和 Recall 的调和平均（harmonic mean）来定义的 $$\frac{1}{F1}=\frac{1}{2}.(\frac{1}{P}+\frac{1}{R})$$ F1 更一般的形式是加权调和平均，它能让我们表达出对 Precision/Recall 的不同偏好，其中 $\beta&gt;0$ 度量了 Recall 对 Precision 的相对重要性 $\beta=1$ 时，退化为标准的 F1 $\beta&gt;1$ 时，Recall 有更大影响 $\beta&lt;1$ 时，Precision 有更大影响 $$\frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}.(\frac{1}{P}+\frac{\beta^2}{R})$$ $$F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P)+R}$$ Macro &amp; Micro Measure很多时候我们有多个二分类混淆矩阵，例如进行多次训练或者在多个数据集上进行训练，此时我们希望计算出全局性能，综合考虑 n 个混淆矩阵上的 Precision 和 Recall 方法 1：先在各混淆矩阵上分别计算 P 和 R，在计算平均值，这样就得到 macro-P、macro-R 和 macro-F1 方法 2：先将各混淆矩阵的对应元素进行平均，再基于这些平均值计算得到 micro-P、micro-R 和 micro-F1 ROC / AUC-ROCROC Curve 很多学习器是为测试样本产生一个实质或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。这样分类过程就相当于在概率值排序中以某个截断点（cut point）将样本分为两部分，前一部分作为正例，后一部分作为反例。 在不同的任务中，我们可以根据任务需求的不同采用不同的截断点，若更重视 Precision，则应选择排序靠前的位置进行截断；如果更重视 Recall，则应选择排序靠后的位置进行截断。 因此，排序本身的质量好坏，体现了综合考虑学习器在不同任务下“期望泛化性能”的好坏，ROC 曲线就是从这个角度研究学习器泛化性能的有力工具。 ROC 的全称是 “受试者工作特征”（Reveiver Operating Characteristic）曲线，源自二战中用于敌机检测的雷达信号分析技术 与 P-R 曲线相似，首先，我们根据学习器的预测结果对样例排序 然后，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要的值，分别作为横纵轴，就得到 ROC 曲线 ROC 曲线的纵轴是“真正例率”（True Positive Rate, TPR），横轴是“假正例率”（False Positive Rate, FPR） $$TPR=\frac{TP}{TP+FN}=Sensitivity$$$$FPR=\frac{FP}{TN+FP}=1-Specificity=1-\frac{TN}{TN+FP}$$ ROC 曲线的对角线对应于 随机猜测 模型，而点 （0,1） 对应于将所有正例排在所有反例之前的 理想模型 现实任务中通常是利用有限个样例来绘制 ROC 图，无法产生光滑的 ROC 曲线，只能绘制出 (b) 中的近似 ROC 曲线 AUC-ROC 进行学习器的比较时，与 P-R 图类似，若一个学习器的曲线被另一个完全包住，则可断定后者优于前者 若两个学习器的曲线发生交叉，则较为合理的判据为 ROC 曲线下的面积，即 AUC-ROC（Area Under ROC Curve） 实际应用中，划分正例和负例的最优阈值为 ROC 曲线上最接近（0,1）的点 Bias-Variance Dilemma“偏差-方差分解”是解释学习算法泛化性能的一种重要工具，泛化误差可以分解为偏差、方差和噪声之和。 偏差：度量学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力 方差：度量了同样大小的训练集的变动所导致的的学习性能的变化，即刻画了数据扰动所造成的影响 噪声：表达了当前任务中任何学习算法所能达到的期望泛化误差的下限，即刻画了学习问题本身的难度 偏差-方差分解说明，泛化性能是由学习算法本身的能力、数据的充分性以及学习任务本身的难度共同决定的。一般来说，偏差与方差是相互矛盾的。 在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率 随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动逐渐被学习器学到，方差逐渐主导泛化错误率 在训练程度充足后，学习器的拟合能力非常强，训练数据发生轻微扰动都会导致学习器发生显著变化，若训练数据自身、非全局的特性也被学习到，则将发生过拟合 Reference 周志华. (2016). 机器学习 : = Machine learning. 清华大学出版社.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[连续型变量的预测误差度量方法]]></title>
      <url>%2F2017%2F04%2F03%2FML3-Prediction-Accuracy-1%2F</url>
      <content type="text"><![CDATA[本文总结讨论了针对连续型、单变量的预测误差度量方法 Let $Y_t$ denote the observation at time $t$ and $F_t$ denote the forecast of $Y_t$. Then define the forecast error $e_t=Y_t-F_t$. Scale-Dependent Measures These are useful when comparing different methods applied to the same set of data, but should not be used. table th:nth-of-type(2) { width: 100px; } Measure Acronym Definition Feature Mean Square Error MSE $mean(e_t^2)$ Root Mean Square Error RMSE $\sqrt{MSE}$ Often, the RMSE is preferred to the MSE as it is on the same scale as the data Mean Absolute Error MAE $mean(abs(e_t))$ Median Absolute Error MdAE $median(e_t)$ Measures Based on Percentage Errors The percentage error is given by $p_t=100e_t/Y_t$. Percentage errors have the advantage of being scale-independent, and so are frequently used to compare forecast performance across different data sets. These measures have the disadvantage of being infinite or undefined if $Y_t=0$ for any $t$ in the period of interest, and having an extremely skewed distribution when any value of $Y_t$ is close to zero. table th:nth-of-type(2) { width: 100px; } Measure Acronym Definition Feature Mean Absolute Percentage Error MAPE $mean(abs(p_t))$ MAPE is often substantially larger than the MdAPE due to the skewed distribution when $Y_t$ is close to zero Median Absolute Percentage Error MdAPE $median(abs(p_t))$ Root Mean Square Percentage Error RMSPE $\sqrt{mean(p_t^2)}$ Root Median Square Percentage Error RMdSPE $\sqrt{median(p_t^2)}$ The MAPE and MdAPE also have the disadvantage that they put a heavier penalty on positive errors than on negative errors. This observation led to the use of the so-called symmetric measures. The problems arising from small values of $Y_t$ may be less severe for sMAPE and sMdAPE. However, even there if $Y_t$ is close to zero, $F_t$ is also likely to be close to zero. Measures based on percentage errors are often highly skewed, and therefore transformations (such as logarithms) can make them more stable. table th:nth-of-type(2) { width: 100px; } Measure Acronym Definition Symmetric Mean Absolute Percentage Error sMAPE $mean(200*abs(Y_t-F_t)/(Y_t+F_t))$ Symmetric Median Absolute Percentage Error sMdAPE $median(200*abs(Y_t-F_t)/(Y_t+F_t))$ Measures Based on Relative Errors An alternative way of scaling is to divide each error by the error obtained using another standard method of forecasting. Let $r_t = e_t / e_t^*$ denote the relative error. where $e_t^*$ is the forecast error obtained from the benchmark method. Measure Acronym Definition Mean Relative Absolute Error MRAE $mean(abs(r_t))$ Median Relative Absolute Error MdRAE $median(abs(r_t))$ Geometric Mean Relative Absoluate Error GMRAE $gmean(abs(r_t))$ Relative Measures Rather than use relative errors, one can use relative measures. For example, let $MAE_b$ denote the MAE from the benchmark method. Then, a relative $MAE$ is given by $RelMAE = MAE/MAE_b$. Similar measures can be defined using RMSEs, MdAEs, MAPEs, etc. When $RelMAE &lt; 1$, the proposed method is better than the benchmark method, and when $RelMAE &gt; 1$, the proposed method is worse than the benchmark method. Percent Better A related approach is to use the percentage of forecasts for which a given method is more accurate than the benchmark method. This is often known as Percent Better and can be expressed as $PB(MAE)=100mean(I(MAE&lt;MAE_b))$ Weighted MeasuresIt is reasonable to assume that every prediction should not be treated equally. For instance, we can assign weights in a way that the higher the weight, the higher importance we are placing on more recent data. The weighted Mean Absolute Error for a recommender system can be computed as following, where $U$ represents the number of users; $N_i$ , the number of items predicted for the $i^{th}$ user; $r_{i,j}$, the rating given by the $i^{th}$ user to the item $I_j$; $p_{i,j}$, the rating predicted by the model; $w_{i,j}$ represents the weight associated to this prediction. There Is Also Another Error Metric ?$$WAPE = 100 \times \frac{sum(abs(Y_t-F_t))}{sum(Y_t)}$$ Scaled Errors By scaling the error based on the in-sample MAE from the naive (random walk) forecast method. Thus, a scaled error is defined as following, which is clearly independent of the scale of the data. A scaled error is less than one if it arises from a better forecast than the average one-step naive forecast computed in-sample. Conversely, it is greater than one if the forecast is worse than the average one-step naive forecast computed in-sample. The Mean Absolute Scaled Error is simply$$MASE=mean(|q_t|)$$ Related measures such as Root Mean Squared Scaled Error (RMSSE) and Median Absolute ScaledError (MdASE) can be defined analogously. Of these measures, we prefer MASE as it is less sensitive to outliers and more easily interpreted than RMSSE, and less variable on small samples than MdASE. Appendix MAPE: Mean Absolute Percentage Error, where $A$ is actual value and $F$ is forecast value.$$MAPE = \frac{100}{n}\sum_{t=1}^n \left | \frac{A_t-F_t}{A_t} \right |$$ RMSPE: Root Mean Square Percentage Error$$RMSPE = \sqrt {\frac{1}{n}\sum_{t=1}^n (\frac{A_t-F_t}{A_t})^2}$$ Reference Hyndman, R. J., &amp; Koehler, A. B. (2006). Another look at measures of forecast accuracy. International journal of forecasting, 22(4), 679-688. Cleger-Tamayo, S., Fernández-Luna, J. M., &amp; Huete, J. F. (2012, September). On the Use of Weighted Mean Absolute Error in Recommender Systems. In RUE@ RecSys (pp. 24-26). WMAPE?, W. (2017). What’s the gaps for the forecast error metrics: MAPE and WMAPE?. Stackoverflow.com. Retrieved 3 April 2017, from http://stackoverflow.com/questions/12994929/whats-the-gaps-for-the-forecast-error-metrics-mape-and-wmape]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Git 版本控制系统常用命令汇总]]></title>
      <url>%2F2017%2F04%2F01%2FTech3-Git-Command%2F</url>
      <content type="text"><![CDATA[Git是目前世界上最先进的分布式版本控制系统（没有之一） 创建版本库 创建版本库：git init 把文件添加到仓库：git add &lt;file&gt; 把文件提交到仓库：git commit -m &lt;message&gt;，-m 是本次提交的说明 版本管理 查看仓库状态：git status 查看具体修改内容：git diff &lt;file&gt; 版本回退HEAD 表示当前版本，HEAD^ 表示上一个版本，HEAD^^上上一个版本，HEAD~100前100个版本 查看历史提交记录：git log / git log --pretty=oneline 版本回退：git reset --hard HEAD 退回到上一个版本git reset --hard HEAD^之后运行git log，已经看不到最新版本的记录了 从新回到最新版本：git reset --hard加上最新版本号的前几位就可以了 显示每一次命令的id：git reflog，方便在任何时间回到任意一个版本 强行将旧版本推送并覆盖远程库：git push -f origin master 工作区和暂存区 工作区：working directory 项目工作的目录 版本库：repository 工作区的.git隐藏目录不算工作区，而是 git 的版本库 git add 命令实际上就是把要提交的所有修改放到暂存区（Stage），然后，执行git commit就可以一次性把暂存区的所有修改提交到分支。没有 git add 之前，文件不会被追踪，也就是不会出现在 git status 中 管理修改 Git 跟踪管理的是修改，而不是文件 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git commit：当你用git add命令后，在工作区的第一次修改被放入暂存区，准备提交，但是，在工作区的第二次修改并没有放入暂存区，所以，git commit只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交 查看工作区和版本库里面最新版本的区别：git diff HEAD --readme.txt 撤销提交到暂存区的内容，重新放回工作区：git rm --cache filename，如果是文件夹再加上 -r 撤销修改 丢弃工作区的修改（add之前）：git checkout -- readme.txt 丢弃暂存区的修改（add之后，commit之前）：git reset HEAD readme.txt 回到add之前的状态 git reset 命令既可以回退版本，也可以把暂存区的修改回退到工作区 修改撤销总结 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 git checkout -- file 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令 git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退2.1节，不过前提是没有推送到远程库 删除文件 在 git 中，删除也是一个修改操作 确实要从版本库中删除该文件，那就用命令git rm file删掉，并且git commit 如果删错了，因为版本库里还有，所以可以很轻松地把误删的文件恢复到最新版本：git checkout -- test.txt。git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以一键还原。 批量删除文件：git add --all，然后 commit 远程仓库 由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要一点设置： 创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key：ssh-keygen -t rsa -C &quot;youremail@example.com&quot;，你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可，由于这个Key也不是用于军事目的，所以也无需设置密码。如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 登陆GitHub，打开“Account settings”，“SSH Keys”页面：然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容，点“Add Key”，你就应该看到已经添加的Key： 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。 添加、删除远程库 登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库 关联本地仓库与远程仓库：git remote add origin git@github.com:jasonlian/learngit.git。添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库 把本地库的内容推送到远程库：git push -u origin master，第一次推送加上-u，以后就不用加了，直接用git push origin master** 把本地分支推送到远程库：git push origin &lt;local branch name&gt;:&lt;remote branch name&gt; 删除远程库：git remote rm origin 从远程库克隆 远程库克隆到本地：git clone git@github.com:jasonlian/gitskills.git 或者 https://github.com/jasonlian/gitskills.git 使用https除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用ssh协议而只能用https 从远程仓库获取更新到本地仓库 查看远程仓库：git remote -v 下载远程库的最新版本到本地新建一个 temp 分支：git fetch origin master:temp 比较本地仓库与分支的差异：git diff temp 合并 temp 分支到本地的 master 分支：git merge temp 删除 temp 分支：git branch -d temp 注意：由于本地仓库可能已经被修改，因此合并远程库的过程中可能会报错，此时有两种处理方法 如果要保留本地修改：直接 commit；或者 git stash + git merge temp + git stash pop，含义就是先将本地修改备份，然后合并，然后再将备份恢复，但是这样做修改内容和远程更新可能会存在冲突生成一系列 bug！ 如果想放弃本地修改：git reset --hard + git merge temp 另外，fetch 命令和 pull 命令的区别是：pull = fetch + merge 提交到多个远程仓库 在每个远程仓库设置公钥之后，直接在本地的 .git 文件夹下的 config 文件中添加远程仓库的地址： [remote &quot;all&quot;] url = https://github.com/abel533/Mapper.git url = https://git.oschina.net/free/Mapper.git 分支管理创建与合并分支 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;被合并分支名 name&gt; 删除分支：git branch -d &lt;name&gt; 解决冲突 当Git无法自动合并分支时（通常是因为在多个分支上同时有修改），就必须首先解决冲突。解决冲突后，再提交，合并完成。 用 git log --graph 命令可以看到分支合并图。 分支管理策略 合并分支时，加上–no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并 git merge --no-ff -m &quot;merge with no-ff&quot; dev 查看分支历史：git log --graph --pretty=oneline --abbrev-commit 创建分支会造成本地查看文件的时候只能同时查看一个分支的文件，不太方便，因此还是使用一个分支吧 Bug 分支 修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除 当手头工作没有完成时，先把工作现场 git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场 查看 stash：git stash list 恢复指定的 stash：git stash apply stash@{0} 丢弃一个没有被合并过的分支：git branch -D &lt;name&gt; 多人协作 查看远程库信息，使用 git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 标签管理 发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针 创建标签 命令git tag &lt;name&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id，通过git log查看 git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息； 命令git tag可以查看所有标签 查看标签详细信息：git show &lt;tagname&gt; 操作标签 删除一个本地标签：git tag -d &lt;tagname&gt; 推送一个本地标签：git push origin &lt;tagname&gt; 推送全部未推送过的本地标签：git push origin --tags 删除一个远程标签：git push origin :refs/tags/&lt;tagname&gt; 使用 Github 在GitHub上，可以任意Fork开源仓库； 自己拥有Fork后的仓库的读写权限； 可以推送pull request给官方仓库来贡献代码。 自定义 Git忽略特殊文件 在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。 不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：https://github.com/github/gitignore 忽略文件的原则是： 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 忽略某些文件时，需要编写.gitignore； .gitignore 文件本身要放到版本库里，并且可以对.gitignore做版本管理！ 配置别名 用st代替status：git config --global alias.st status 也可以在 .git/config 文件中直接修改 Reference Github - Git Cheet Sheet Pro Git 简体中文版. (2017). Iissnan.com. Retrieved 1 April 2017 Git教程. (2017). Liaoxuefeng.com. Retrieved 1 April 2017 Git - Documentation. (2017). Git-scm.com. Retrieved 1 April 2017]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习中的特征选择方法概述]]></title>
      <url>%2F2017%2F03%2F13%2FML2-Feature-Selection%2F</url>
      <content type="text"><![CDATA[Feature Selection 是在模型构建过程中选择最相关、最有利于提高预测效果的特征子集的过程 什么是特征选择 机器学习中的特征选择（Feature Selection）也被称为 Variable Selection 或 Attribute Selection 虽然特征选择和降维（dimensionality reduction）都是为了减少特征的数量，但是特征选择不同于降维 降维是创造特征的新组合，比如 PCA 和 SVD 特征选择则只是从原有特征中进行选择或排除，不涉及原有特征的转变 为什么需要特征选择 在训练机器学习模型之前，特征选择是一个很重要的预处理过程，之所以进行特征选择，有以下几点很重要的原因 现实任务中经常遇到维数灾难问题，如果能选择出重要特征，再进行后续学习过程，则维数灾难可以大为减轻 去除不相关的特征往往会降低学习任务的难度，使模型更易理解（比如，使决策树的规则变得更加清晰） 去除不相关的变量还可以尽量减少过拟合的风险，尤其是在使用人工神经网络或者回归分析等方法时，额外的输入变量会增加模型本身的额外自由度，这些额外的自由度对于模型记住某些细节信息会有所帮助，但对于创建一个稳定性良好、泛化性能强的模型却没有好处，也就是说增加额外的不相关变量会增大过拟合的风险（决策树技术是不存在过拟合风险的一个例子，因为标准的决策树一次只选取一个变量） 特征选择的两个关键环节 想要从初始的特征集合中选取一个包含所有重要信息的特征子集，若没有任何先验知识，则只能遍历所有可能的子集，然而这样在计算上显然不可能，尤其是在特征个数很多的情况下 可行的方法是：产生一个候选子集，评价它的好坏，基于评价结果产生下一个候选子集，再对其进行评价……持续这一过程，直到找不到更好的子集为止 这一过程涉及到两个关键环节：如何根据评价结果获取下一个特征子集？如何评价候选特征子集的好坏？ 环节一：子集搜索问题 前向搜索：第一个环节是“子集搜索”问题，给定特征集合 ${a_1,a_2,…,a_n}$，首先选择一个最优的单特征子集（比如 ${a_2}$）作为第一轮选定集，然后在此基础上加入一个特征，构建包含两个特征的候选子集，选择最优的双特征子集作为第二轮选定子集，依次类推，直到找不到更优的特征子集才停止，这样逐渐增加相关特征的策略成为前向（forward）搜索 后向搜索：类似的，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为后向（backward）搜索 双向搜索：前向后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中确定不会被去除），同时减少无关特征，这样的策略被称为双向（bidirectional）搜索 上述策略都是贪心策略，仅考虑本轮选定集最优，但若不进行穷举，这样的问题就无法避免 环节二：子集评价问题 确定了搜索策略，接下来就需要对特征子集进行评价，以离散型属性的信息增益为例 给定数据集 $D$，假定 $D$ 中第 $i$ 类样本的比例为 $p_i (i=1,2,…,n)$，则信息熵的定义为: $$Ent(D)=-\sum_{i=1}^n p_k log_2 p_k$$ 对于属性子集 $A$，假定根据其取值将 $D$ 分成了 $V$ 个子集 ${D^1,D^2,…,D^V}$，每个子集的样本在 $A$ 上取值相同，于是我们可以计算属性子集 $A$ 的信息增益为：$$Gain(A)=Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} Ent(D^v)$$ 信息增益越大，意味着特征子集 $A$ 包含的有助于分类的信息越多。于是，对于每个特征子集，我们可以基于训练集 $D$ 来计算其信息增益，以此作为评价标准 主要的 特征/特征子集 评价方法（Selection）如下 特征子集搜索机制和子集评价机制相结合，就可以得到特征选择方法。例如，将前向搜索和信息熵相结合就和决策树算法非常相似（不同之处是从第二步开始，决策树是在每个孩子节点的数据集上评价特征，而前向搜索依然是在整个数据集上评价特征）。事实上决策树本身就是一种特征选择的方法，树节点的划分属性组成的集合就是选择出的特征子集！ 常见的特征选择方法 常用的特征选择方法大致可以分为三类：过滤式（filter）、包裹式（wrapper）和嵌入式（embedding） Filter Method 过滤式方法先对数据集进行特征选择，然后再训练模型，特征选择过程与后续模型训练无关 Filter 方法常用的特征子集评价标准包括：相关系数、互信息、信息增益等 更多方法参见 mlr 包支持的所有 Filter 方法 Wrapper Method 与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的模型的性能作为特征子集的评价标准，也就是说，包裹式特征选择的目的就是为给定的模型选择最有利于其性能的特征子集 从最终模型的性能来看，包裹式特征选择比过滤式特征选择更好，但需要多次训练模型，因此计算开销较大 Filter 和 Wrapper 方法的区别如下 Embedding Method 在前两种特征选择方法中，特征选择过程和模型训练过程是有明显分别的两个过程 而嵌入式特征选择是将特征选择过程与模型训练过程融为一体，两者在同一个优化过程中完成，即在模型训练的过程中自动进行特征选择，嵌入式选择的实例是 LASSO 和 Ridge Regression 由于决策树算法在构建树的同时也可以看作进行了特征选择，因此嵌入式方法可以追溯到 ID3 算法 在 R 中实现特征选择 R 中的具体实现参见 Filter &amp; Wrapper Method in mlr Package PCA / Feature Important / Correlation / Recursive Feature Elimination / Genetic Algorithm Reference 周志华. (2016). 机器学习 : = Machine learning. 清华大学出版社. Gordon S. Linoff, &amp; Michael J.A. Berry. (2013). 数据挖掘技术. 清华大学出版社. A short tutorial on Feature Selection An Introduction to Feature Selection]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[利用高斯过程回归进行时间序列预测]]></title>
      <url>%2F2017%2F03%2F08%2FML1-GPR-for-Prediction%2F</url>
      <content type="text"><![CDATA[本文介绍了高斯回归的基本概念，并在 R 中利用高斯回归实现对时间序列的预测 为什么要用高斯过程回归 现实生活中，我们遇到的一个典型问题就是选择合适的模型拟合训练集中自变量 $X$ 与因变量 $y$ 之间的关系，并根据新的自变量 $x$ 来预测对应的因变量 $f$$$p(f|x,X,y)$$ 如果关系足够简单，那么线性回归就能实现很好的预测，但现实情况往往十分复杂，此时，高斯过程回归就为我们提供了拟合复杂关系（quadratic, cubic, or even nonpolynomial）的绝佳方法 什么是高斯过程回归 高斯过程可以看做是多维高斯分布向无限维的扩展，我们可以将 $y={y_1,y_2,…,y_n}$ 看作是从 $n$ 维高斯分布中随机抽取的一个点 对高斯过程的刻画，如同高斯分布一样，也是用均值和方差来刻画。通常在应用高斯过程 $f \sim GP(m,K)$ 的方法中，都是假设均值 $m$ 为零，而协方差函数 $K$ 则是根据具体应用而定 高斯回归的本质其实就是通过一个映射把自变量从低维空间映射到高维空间（类似于支持向量机中的核函数将低维线性不可分映射为高维线性可分），只需找到合适的核函数，就可以知道 $p(f|x,X,y)$ 的分布，最常用的就是高斯核函数 高斯过程回归的基本流程 再利用高斯过程回归时，不需要指明 $f(x)$ 的具体形式，如线性 $f(x)=mx+c$，或者二次 $f(x)=ax^2+bx+c$ 等具体形式，n 个训练集的观测值 ${y_1,y_2,…,y_n}$ 会被看做多维（n 维）高斯分布中采样出来的一个点 现在给定训练集 ${x_1,x_2,…,x_n}$ 与对应的观测值 ${y_1,y_2,…,y_n}$，由于观测通常是带噪声的，所以将每个观测 $y$ 建模为某个隐函数 $f(x)$ 加上一个高斯噪声，即$$y=f(x)+N(0,\sigma_n^2)$$ 其中，$f(x)$ 被假定给予一个高斯过程先验，即$$f(x) \sim GP(0,K)$$ 其中协方差函数 $k(x,x’)$ 可以选择不同的单一形式，也可以采用协方差函数的组合形式，由于假设均值为零，因此最后结果的好坏很大程度上取决于协方差函数的选择。不同的协方差函数形式参见这篇文章对 Covariance Functions 的详细介绍。常见的协方差函数如下，参见 Wikipedia-Gaussian Process 根据高斯分布的性质以及测试集和训练集数据来自同一分布的特点，可以得到训练数据与测试数据的联合分布为高维的高斯分布，有了联合分布就可以比较容易地求出预测数据 $y^\ast$ 的条件分布 $p(y^\ast|y)$，对 $y^\ast$ 的估计，我们就用分布的均值来作为其估计值，具体推导参见 Reference 利用高斯过程进行时间序列预测 R 中 kernlab 包的 gausspr 函数可以进行高斯回归，并实现预测，以下面这个包含 46 个月的时间序列 ts7 为例 利用趋势回归并进行预测12345678910111213141516library(kernlab)library(ggplot2)library(gplots)library(forecast)library(data.table)library(tidyr)library(plotly)temp &lt;- data.table(ts7)fit &lt;- gausspr(demand~t, data=temp)temp$fitted &lt;- predict(fit, temp[,.(t)])ggplot(temp, aes(x=year_month, group=1)) + geom_line(aes(y=demand, col="demand"), size=1) + geom_line(aes(y=fitted, col="fitted"), size=1) + theme_bw() + theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1)) + scale_x_discrete(breaks=temp$year_month[seq(2,44,3)]) 只利用趋势项进行高斯回归的拟合效果如下 然后用过去三年的时间序列作为训练集对未来一个月的需求进行循环预测 123456temp1 &lt;- data.table(ts7, fitted=0)for (k in 0:9)&#123; train &lt;- temp1[(1+k):(36+k),3:4] fit &lt;- gausspr(demand~t, data=train) temp1[(37+k), "fitted"] &lt;- predict(fit, temp1[(37+k),.(t)])&#125; 利用趋势+季节回归并进行预测 首先，去除趋势之后，检查去趋势之后的时间序列是否具有明显的季节性，并找出 CV 最小的前三个季节12temp$demand_detrend &lt;- temp$demand - temp$fittedggplot(temp, aes(x=year_month, group=1)) + geom_line(aes(y=demand, col="demand"), size=1) + geom_line(aes(y=fitted, col="fitted"), size=1) + geom_line(aes(y=demand_detrend, col="demand_detrend")) + theme_bw() + theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1)) + scale_x_discrete(breaks=temp$year_month[seq(2,44,3)]) 季节性雷达图1ggseasonplot(temp$demand_detrend, polar=TRUE) + ggtitle("Seasonal Plot") + geom_line(size=1) + theme_bw() 季节性箱形图1ggplot(temp, aes(x=month, y=demand_detrend)) + geom_boxplot() + theme_bw() 获取季节 cv 最小的前 3 个季节分别是12月、2月、10月 12as.numeric(temp[, .(cv=sd(demand_detrend)/mean(abs(demand_detrend))), by=month][order(cv)][1:3,month])# [1] 12 2 10 加入全部 12 个月作为季节性之后，再对最后的 10 个月进行循环预测 123456temp2 &lt;- data.table(ts7, fitted=0)for (k in 0:9)&#123; train &lt;- temp2[(1+k):(36+k),3:15] fit &lt;- gausspr(demand~., data=train) temp2[(37+k), "fitted"] &lt;- predict(fit, temp2[(37+k),4:15])&#125; 预测结果比较1234567891011121314temp &lt;- data.frame(temp1[,c(1:3,16)], temp2[,16])colnames(temp)[3:5] &lt;- c("Actual_demand","Predict_trend","Predict_trend+seasonal")temp[4:5] &lt;- round(temp[4:5], 2)temp[temp==0] &lt;- NAtemp &lt;- gather(temp[,-2], key="Series", value="value", -year_month)p &lt;- ggplot(temp, aes(x=year_month, y=value, group=Series, col=Series)) + geom_line(size=1) + geom_point() + theme_bw() + theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1)) + scale_x_discrete(breaks=temp$year_month[seq(2,44,3)])postlink &lt;- plotly_POST(p, filename = "GPR Prediction Example")postlink 总结 当随机变量呈现明显的非线性趋势时，高斯过程回归能够很好地预测线性预测的不足 季节性并不一定能够提高预测效果，当某些月份的需求变动幅度很大时，加入季节虚拟变量反而会增大预测误差 高斯过程不仅能用于回归预测，还能用于解决分类问题，有兴趣的读者请自行探究 Reference Gaussian Process for Regression_A quick introduction Introduction to Gaussian Processes The Gaussian Processes Web Site gausspr: Gaussian processes for regression and classification predict.gausspr: predict method for Gaussian Processes object 高斯过程回归？效果意想不到的好 高斯过程回归简介_学习笔记]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在 R 中使用 Plotly 创建交互式动态图表]]></title>
      <url>%2F2017%2F03%2F06%2FData1-Plotly%2F</url>
      <content type="text"><![CDATA[Plotly 是一款在线分析和数据可视化工具，并为 Python、R、MATLAB 等主流编程语言提供 API 接口 在 R 中安装 Plotly 通过 CRAN 安装：install.packages(&quot;plotly&quot;) 通过 Github 安装最新版本：devtools::install_github(&quot;ropensci/plotly&quot;) 用 plot_ly 绘制离线图表 通过 plot_ly() 函数直接绘制离线动态图表，然后在 Rstudio Viewer 中查看12library(plotly)plot_ly(midwest, x = ~percollege, color = ~state, type = "box") 生成在线动态图表 在 Plotly 的官网注册一个账号 在设置中生成一个 API Key，并在 R 中进行如下设置 12Sys.setenv("plotly_username"="your_plotly_username")Sys.setenv("plotly_api_key"="your_api_key") 为了避免每次打开一个 R Session 都要重新进行以上配置，可以将这两行代码添加到 R 安装目录的 \etc\Rprofile.site 文件中，这样每次开启 R 就会自动进行配置 设置好之后，通过 plotly_POST 命令将绘图上传至 Plotly 网站，效果参见此链接12p &lt;- plot_ly(midwest, x = ~percollege, color = ~state, type = "box")plotly_POST(p, filename = "midwest-boxplots") 将 Plotly 嵌入 Html 中 点击 Plotly 图表上的分享按钮，复制 Embed 下的 iframe 或 Html 代码，嵌入网页源码即可，或者直接写入 markdown 中 居中图片使用 &lt;center&gt; 标签，宽度和高度也可以在 iframe 中直接设置，效果如下 iframe 的配置参数如下表 将 ggplot2 的绘图转换成 Plotly123456789101112131415library(ggplot2)library(plotly)set.seed(100)d &lt;- diamonds[sample(nrow(diamonds), 1000), ]p &lt;- ggplot(data = d, aes(x = carat, y = price)) + geom_point(aes(text = paste("Clarity:", clarity)), size = .5) + geom_smooth(aes(colour = cut, fill = cut)) + facet_wrap(~ cut)p &lt;- ggplotly(p)# Create a shareable link to your chartchart_link = plotly_POST(p, filename="ggplot2-plotly-example")chart_link &gt; 将 Plotly D3.js 插入 Powerpoint/Excel 通过 Office 应用商店安装 Plotly D3.js 插件 在 Powerpoint/Excel 中通过加载项插入 Plotly 图标的 https 分享链接即可 Reference Get started with plotly for R How to Embed Graphs in a Blog or Website Plotly ggplot2 Library]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo 博客的 Next 主题配置]]></title>
      <url>%2F2017%2F02%2F22%2FTech2-Windows-Hexo2%2F</url>
      <content type="text"><![CDATA[NexT 主题拥有丰富而简单的配置，结合第三方服务，可以方便快捷地打造个性化的 Hexo 博客 更换主题 下载 Github 上的主题：git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改配置文件：通过修改 _config.yml 文件中的 theme: yilia 更改主题 查看主题效果：hexo server，注意每次更改预览之前必须先使用 hexo generate 生成网页 设置阅读全文 推荐方法：通过在文章中手动添加 &lt;!-- more --&gt; 来进行截断 也可以在主题配置文件中通入如下代码自动生成摘要123auto_excerpt: enable: false length: 150 添加统计功能 添加文章阅读量统计 在 LeanCloud 中创建名为 Counter 的应用 然后将应用的 App ID 和 App Key 填入主题配置文件中，重新生成部署即可 注意一定要将自己的网址添加到：设置-安全中心-安全域名 添加站点浏览量统计: 将主题设置文件中的 busuanzi_count 设置为 enable: true，然后分别进行以下配置 1234567891011busuanzi_count:# count values only if the other configs are falseenable: true# custom uv span for the whole sitesite_uv: true site_uv_header: &lt;i class="fa fa-user"&gt; 本站访客数&lt;/i&gt; site_uv_footer: 人次# custom pv span for the whole sitesite_pv: true site_pv_header: &lt;i class="fa fa-eye"&gt; 本站总访问量&lt;/i&gt; site_pv_footer: 次 自定义站点内容搜索 首先在博客根目录安装插件 npm install hexo-generator-searchdb --save 然后在站点配置文件添加如下代码 12345search: path: search.xml field: post format: html limit: 10000 最后，编辑主题配置文件，启动本地搜索功能 123# Local searchlocal_search: enable: true 设置 RSS 首先安装插件 npm install hexo-generator-feed --save 然后在主题配置文件中插入如下代码123456feed: type: atom path: atom.xml limit: 20 hub: content: 搜索引擎优化 Google Webmaster Tools 登陆Google 的站点管理工具，导航到验证方法，选择 HTML Tag，将会获取一段代码 &lt;meta name=&quot;google-site-verification&quot; content=&quot;XXXXXXXXXXXXXXXXXXXXXXX&quot; /&gt; 将 content 里面的内容复制出来，并在主题配置文件中新增字段 google_site_verification: XXXXXXXXXXXXXXXXXXXXXXX，重新编译发布后，到 Google 站点管理工具进行验证即可 百度站长工具 进入百度站长工具——官网保护——添加网站 然后获取 HTML 标签，参加 Google Webmaster 将相关标签加入主题配置文件中 baidu_site_verification: XXXXXX 如果网站验证出现 301 错误，这是因为 https 加密导致重定向失败，此时必须使用 CNAME 验证解析，具体方法是在万网中添加解析，记录类型选择 CNAME，主机记录为百度验证码加域名，记录值为 zz.baidu.com，然后提交即可 为文章添加阴影效果 将 themes/next/source/css/_schemes/Mist/_posts-expanded.styl 文件中的 .post{} 修改为：1234567891011.posts-expand &#123; ... .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .8); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .8); &#125; ... &#125; 内置标签文本居中的引用123456789&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;&lt;!-- 其中 class="blockquote-center" 是必须的 --&gt;&lt;blockquote class="blockquote-center"&gt;blah blah blah&lt;/blockquote&gt;&lt;!-- 标签 方式，要求版本在0.4.5或以上 --&gt;&#123;% centerquote %&#125;blah blah blah&#123;% endcenterquote %&#125;&lt;!-- 标签别名 --&gt;&#123;% cq %&#125; blah blah blah &#123;% endcq %&#125; 突破容器宽度限制的图片123456789&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;&lt;!-- 其中 class="full-image" 是必须的 --&gt;&lt;img src="/image-url" class="full-image" /&gt;&lt;!-- 标签 方式，要求版本在0.4.5或以上 --&gt;&#123;% fullimage /image-url, alt, title %&#125;&lt;!-- 别名 --&gt;&#123;% fi /image-url, alt, title %&#125; Bootstrap Calloutclass_name 可以是如下几种:1&#123;% note class_name %&#125; Content &#123;% endnote %&#125; default primary success info warning danger Reference Next 主题官网 Next 主题 Github 主页 Hexo 主题推荐 知乎 Hexo 多终端同步管理 利用 Leancloud 添加文章阅读量统计功能 Hexo 集成多说评论 + 多说分享 + 美化多说 为文章添加阴影效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在 Windows 上利用 Github Page 快速搭建 Hexo 博客]]></title>
      <url>%2F2017%2F02%2F18%2FTech1-Windows-Hexo1%2F</url>
      <content type="text"><![CDATA[Hexo 是一个快速、简单且功能强大的 Node.js 静态博客框架，可以方便地生成静态网页并托管在 Github 等平台上 准备工作 安装 Node.js：到 Node.js 的官网下载最新版本并安装 安装 Git：到 Git 官网下载最新版本安装，然后配置本地 Github 的账户密码并生成密钥 安装 Hexo：在命令行使用 npm install -g hexo 命令安装 Hexo，然后运行 hexo -v，如果输出版本号即为安装成功 在 Github 上创建用于托管博客的仓库：JasonLian.github.io 初始化并生成静态页面 初始化：在本地新建一个项目文件夹（比如 Blog），进入 Blog 目录，运行 hexo init 进行初始化 生成静态页面：运行 hexo generate 将文章编译为静态页面 本地启动：运行 hexo server，然后在浏览器中输入 http://localhost:4000/ 查看生成的页面效果 部署到 Github 部署之前先修改配置文件 1234deploy: type: git repository:git@github.com:JasonLian/JasonLian.github.io.git branch: master 然后，通过 npm install hexo-deployer-git --save 安装部署插件，并进行部署 hexo deploy 部署成功之后，稍等片刻就能通过 jasonlian.github.io 查看博客了，效果和本地预览相同 多终端同步管理 在一台新终端上建立 Blog 文件夹并初始化 Hexo （init &amp; generate） 在 Blog 下新建 .deploy_git 文件夹，进入并运行 1234git initgit remote add origin git@github.com:JasonLian/JasonLian.github.io.gitgit fetch --allgit reset --hard origin/master 一种更简单的方式是：直接使用 Resilio Sync 等同步工具将整个 Blog 文件夹在不同终端之间备份即可 发表文章并添加分类和标签 新建一篇文章：运行 hexo new &quot;title&quot;，然后就可以在 source/_post 文件夹下看到新建的 markdown 文件 发布一篇文章：编辑新建的 md 文件后，generate + deploy 即可 添加标签页面：hexo new page tags 新建标签页面，在标签 md 文件中添加 type: &quot;tags&quot;，然后在主题配置文件的 menu 下添加 tags: /tags 添加分类页面：hexo new page categories 新建标签页面，在标签 md 文件中添加 type: &quot;categories&quot;，然后在主题配置文件的 menu 下添加 categories: /categories 为文章添加分类和标签：在文章的 front-matter 中添加如下代码，注意 Hexo 中每篇文章可以指定多个标签，但是只能指定一个分类，如果需要关闭评论功能，可以将 comments 设置为 false1234categories: Diarytags: - PS3 - Games 将 Hexo 同时托管到 Github 和 Coding Github 在国内的访问速度有时会很慢，甚至出现打不开的情况，而且受 Github 屏蔽百度爬虫所限，托管在 Github Page 的博客在百度上是搜索不到的，除非想百度手动推送你的链接，很是麻烦 因此，可以考虑将 Hexo 托管到 Coding Page 上，步骤和 Github 大致相同，在 Coding 上部署公钥，然后在 Git Bash 中输入命令 ssh -T git@git.coding.net 即可 更进一步，如果想同时托管到 Github Page 和 Coding Page，只需要在万网买一个域名，同时绑定两个网址，并设置国内 ip 指向 Coding，国际 ip 指向 Github 即可，详细步骤参见 Reference 实现简单的文章加密访问 如果使用 next 主题，则找到 themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig 文件，在文件末尾加入以下代码： 12345678910&lt;script&gt; (function()&#123; if('&#123;&#123; page.password &#125;&#125;')&#123; if (prompt('请输入文章密码','') !== '&#123;&#123; page.password &#125;&#125;')&#123; alert('密码错误！'); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; 加入上述代码后，只需在需要加密的文章头部添加 password 变量即可，变量值设置为自己设定的密码 123456789title: Hexo文章简单加密访问date: 2016-12-01 10:45:29tags: hexocategories: 博客keywords: - Hexo - 加密description: 阅读本文，请输入密码password: youpassword 如果你开启了 RSS 订阅功能，或者本地搜索功能，那么在执行 Hexo generate 之后，加密文章的内容也会自动写入 public 文件夹下的 search.xml 文件和 atom.xml 文件中 为了去除这些文件中的敏感内容，在博客根目录新建一个 rm_secret.py 文件，添加如下代码，在每次 generate 和 deploy 之间，将加密文章名添加到 delete_article_names 数组中，然后执行即可123456789101112131415161718192021222324# coding:utf8import re# 设置需要去除的文章名delete_article_names = ["article1", "article2"]delete_files = ['./public/atom.xml', './public/search.xml']for delete_file in delete_files: fr = open(delete_file) content = fr.read() # print content fr.close() checks = re.findall(r'&lt;entry&gt;(.*?)&lt;/entry&gt;', content, re.S) for check in checks: delete = 0 for delete_article_name in delete_article_names: if delete_article_name in check: delete = 1 if delete==1: content = content.replace('&lt;entry&gt;' + check + '&lt;/entry&gt;', '') fw = open(delete_file, 'w') fw.write(content) fw.close() 常见错误及解决方案hexo deploy 部署失败 部署时提示如下错误，解决方法为删除 .deploy_git 文件夹后重新部署123456789101112error: bad signaturefatal: index file corruptFATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: error: bad signaturefatal: index file corrupt at ChildProcess.&lt;anonymous&gt; (C:\Users\Lian\Blog\node_modules\hexo-util\lib\spawn.js:37:17) at emitTwo (events.js:106:13) at ChildProcess.emit (events.js:191:7) at ChildProcess.cp.emit (C:\Users\Lian\Blog\node_modules\cross-spawn\lib\enoent.js:40:29) at maybeClose (internal/child_process.js:877:16) at Process.ChildProcess._handle.onexit (internal/child_process.js:226:5) Reference Hexo 官网 Hexo vs. Jekyll：配置 Hexo 20 分钟教你使用 hexo 搭建 github 博客：配置 + 发布文章 Github Page + Hexo 搭建博客 将hexo博客同时托管到github和coding Hexo 文章简单加密访问]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用的可视化资源导航]]></title>
      <url>%2F2017%2F02%2F16%2FNavi3-Visualization%2F</url>
      <content type="text"><![CDATA[持续更新：R 中常用的可视化资源 Awesome Website ggplot2 Docs ggplot2 Cheatsheet ggplot2 Rmd: A Tutorial for R ggplot2 ggplot2 Plotting System ggplot() 中控制的是总体设置，比如 ggplot() 中的 color 决定了所有 geom 的颜色，而 geom 中的设置只是针对各个图 凡是涉及到与变量相关的设置，必须用 aes() Geomsgeom_bar (x is discrete) Bar and Line Graph_Cookbook for R/) Bars, rectangles with bases on x-axis 使用 ggplot2 绘制条形图 按照y轴从大到小的顺序排列bar的分布：aes(x=reorder(x_name, -value), y=value, fill=name geom_bar(stat=&quot;identity&quot;, position=position_dodge(0.7)) 默认的 position 是堆叠 stack，百分比堆叠是 position=&quot;fill&quot;, 而按政府值区分开是 position=&quot;identity&quot; 使用明细数据计数之后绘制条形图：ggplot(data, aes(x=gender)) + geom_bar(stat=&quot;count&quot;, width=.3, col=&quot;orange&quot;) stat 的默认值是 count 对于汇总好的数据：geom_bar(stat = &quot;identity&quot;) 添加标签：geom_text(stat=&#39;count&#39;, aes(label=..count..), vjust=-0.5, hjust=0.5, size=3, colour=&quot;orange&quot;) 如果是堆叠式，则加入 position=position_stack() geom_histogram (x is continuous) Make a histogram with ggplot2 qplot(chol$AGE, geom=&quot;histogram&quot;, binwidth = 5, main = &quot;Histogram for Age&quot;, xlab = &quot;Age&quot;, fill=I(&quot;blue&quot;), col=I(&quot;red&quot;), alpha=I(.2), xlim=c(20,50)) ggplot(data, aes(x=duration)) + geom_histogram(binwidth=60) How to make a histogram 添加计数的标签：+ stat_bin(binwidth=0.5, geom=&quot;text&quot;, aes(label=..count..), vjust=-0.5, size=3) geom_densitygeom_linegeom_point 使用 ggplot2 绘制散点图 使用GGally包中的ggpairs()函数绘制散点图矩阵：library(GGally) ggpairs(tips[, 1:3]) geom_boxplot 画箱形图，x 为分组变量，y 为值：ggplot(result_all, aes(x=factor(cluster), y=WAPE_min)) + geom_boxplot() 添加均值：stat_summary(fun.y=mean, colour=&quot;blue&quot;, geom=&quot;point&quot;, size=2) + stat_summary(fun.y=mean, colour=&quot;red&quot;, geom=&quot;text&quot;, vjust=0.3, hjust=-0.5, size=3, aes( label=round(..y.., digits=2))) geom_text 添加标签，避免文字重叠：geom_text(aes(label=value), position=position_dodge(width=0.9), vjust=-0.25) geom_text(aes(label = Frequency), size = 3, hjust = 0.5, vjust = 3, position = &quot;stack&quot;) stack 表示堆叠显示 geom_vline 添加一条垂直线：geom_vline(xintercept=as.numeric(as.Date(&quot;1959-12-01&quot;)), linetype=2) geom_hline 添加一条水平线：geom_hline(yintercept=200) geom_ribbon 作缎带图：geom_ribbon(aes(ymin=LL, ymax=UL), fill=&quot;grey&quot;, alpha=0.5) geom_rug 向散点图添加坐标轴边际地毯：geom_rug(position = &#39;jitter&#39;, size = 0.1) geom_label_repel &amp;&amp; geom_text_repel 来自 ggrepel 包，添加标签，geom_label_repel(aes(label=method),size=3) geom_dl 来自 directlabels 包，在线的两端标注分类名称，geom_dl(aes(label = method), method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;), cex = 0.8)) 自动选择最优位置标注：method=&quot;smart.grid&quot; annotate12345678910# 添加文字annotate("text", x = 4, y = 25, label = "Some text")annotate("text", x = 2:5, y = 25, label = "Some text")annotate("text", x = 2:3, y = 20:21, label = c("my label", "label 2"))# 添加矩形框annotate("rect", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2)# 根据起始点画直线annotate("segment", x = 2.5, xend = 4, y = 15, yend = 25, colour = "blue") Statsstat_smooth 向散点图添加拟合线：stat_smooth(method=&quot;lm / loess&quot;) Facetsfacet_wrap facet_wrap(~group_variable，scales=&#39;free&#39;, nrow=2) scales=&#39;free&#39; 表示允许不同的 facet 有不同的 x/y axis limit，如果只允许 x axis，则 scale=‘free_x’ nrow 设置 plot 的行数 facet_grid 在同一个 facet 上作不同类型的图形：ggplot() + geom_line(data=temp[measure %in% c(&quot;MAPE_in&quot;,&quot;MAPE_out&quot;,&quot;CV&quot;)], aes(x=atc_class,y=value,group=measure,color=measure), size=1) + geom_bar(data=temp[measure==&quot;volume&quot;],aes(atc_class,value),stat=&quot;identity&quot;) + facet_grid(group~.,scales=&quot;free_y&quot;) Scales Aesthetic mapping (i.e., with aes()) only says that a variable should be mapped to an aesthetic. It doesn’t say how that should happen.scale_fill_brewer &amp; scale_fill_manual 颜色填充/) scale_fill_brewer(palette=&quot;Set1&quot;) scale_fill_manual(values=c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) scale_shape_manual 自动指定点形状的上限是6，超过则需要手动指定点的形状：scale_shape_manual(values=1:10) scale_color_brewer &amp; scale_color_manual 设置连续颜色变化：scale_colour_gradient(low = &#39;lightblue&#39;, high = &#39;darkblue&#39;) scale_size scale_size_continuous(breaks = c(100,150,200,300,350,400), guide = guide_legend()) 连续变量的大小与形状的大小成比例：scale_size_area(max_size = 10) scale_x_continuous 改变X轴的坐标间隔：scale_x_continuous(breaks=seq(0, max(abc$cum_volume), by=max(abc$cum_volume)/5)) scale_size_manual 将大小分成两组，并手动改变线条粗细：geom_line(aes(col=method, size=(method==&quot;volume&quot;))) + scale_size_manual(values = c(1, 2), name = &quot;&quot;, labels = c()) Point Shape Color NamesThemes 更改 facet 中坐标轴 label 文字的大小：theme(strip.text.y = element_text(size=8)) 文字竖向：theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1)) 标题：labs(list(title = “Cluster Result”, x = “Clustering Variables”, y = “Centroid”)) 标题居中：theme(plot.title = element_text(hjust = 0.5)) 将 Theme 的各元素设置为空： 123456789101112theme(axis.line=element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position="none", panel.background=element_blank(), panel.border=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), plot.background=element_blank()) 白色背景：theme_bw() 设置图标题：ggtitle(paste0(&quot;BSTS Holdout MAPE = &quot;, round(100*MAPE,2), &quot;%&quot;)) 更改标题文字大小：theme(plot.title = element_text(size=10)) 转换坐标轴：coord_flip() OtherLegend Legend Guide Guide：legend 文字更改或者分类维数合并参见 guide Legend 中的分类变量排序：ts$category &lt;- factor(ts$category, levels=c(&quot;vegetable&quot;,&quot;fruit&quot;,&quot;meat&quot;,&quot;total&quot;)) 修改 legend 的标题：scale_color_discrete(guide = guide_legend(title = &quot;Cluster&quot;))，注意 scale 要对应 legend 的维度！比如 aes(color= ) 的 legend 要对应 scale_color_discrete！ Save plot to disk 注意 for 循环内的 plot 要加上 print() 才能输出123png(filename=paste('./png/',i,'.png',sep=''))print(qplot(na.omit(result_boot[,i]), geom='histogram', binwidth=0.1, xlim=c(-20,20), col='red'))dev.off() ggfortify Define fortify and autoplot functions to allow ggplot2 to handle some popular R packages. github Chinese introducton wechat introduction article corrplot An Introduction to Corrplot Package corrplot(cor(dataframe), method = &quot;pie&quot;, type = &quot;lower&quot;, tl.col=&quot;black&quot;, tl.cex=0.7, tl.srt = 20, order=&#39;hclust&#39;) Time Series Plot plot.ts()：时序图 ts.plot()：在同一张图上作多个时序图，ts.plot(ts,train_fit,test_fit, gpars = list(col=c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;))) forecast package tsdisplay() ：时序图+ACF+PACF plot.forecast()：作预测图 Table Plot1234library(gridExtra)library(grid)d &lt;- head(iris[,1:3])grid.table(d) Plotly Blog: Use Plotly to Create Interactive Graph in R]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R 语言实用语法和常用资源速查手册]]></title>
      <url>%2F2017%2F02%2F14%2FNavi2-R-Manual%2F</url>
      <content type="text"><![CDATA[持续更新：R 语言的实用语法和常用资源 Awesome Website Awesome R: A list of awesome R packages and tools METACRAN: Search and browse all CRAN/R packages RDocumentation: Search all CRAN, BioConductor and Github packages R Packages: A comprehensive index of R packages and documentation CRAN Task Views Useful Command 搜索某个包下的指定函数：?proxy::dist 查看包的帮助手册：browseVignettes(package = &quot;dplyr&quot;) 读取文件夹下的所有指定类型的文件：list.files(&quot;../data_survey/&quot;, pattern=&quot;*.xls&quot;) 打开新窗口用于存储新图形：dev.new() 查看变量类型：mode() 查看变量的种类个数：table() 变量 0-1 标准化：scale() 删除所有包含 temp 的对象：rm(list=ls()[grepl(&#39;temp&#39;,ls())]) 查看和清空内存：gc(reset=TRUE) 分类汇总，根据 position 对 a1\a2\a3 分别求和：aggregate(datos[,c(&quot;a1&quot;,&quot;a2&quot;,&quot;a3&quot;)], by=list(datos$Position), &quot;sum&quot;) 根据多列取不重复的子集：df[!duplicated(df[,1:2]),] Array 去除数组两端的 0 值：x[min(which(x!=0)) : max(which(x!=0))] 计算一个向量的累积求和或累积求积 cumsum(c(1,2,3)) →1,3,6 cumprod(c(1,2,3))→1,2,6 比较两个向量是否完全相同：identical(vector1, vector2) 自动产生向量的下标：seq_along() List 选择列表的多个元素：mylist[c(1,3,5)]，而双中括号选择的是再下一层 R 中字典的概念 list(a = 1,b = &quot;foo&quot;,c = 1:5) list[[&#39;a&#39;]] 返回 1 Data Frame 将 data.frame 的所有 NA 替换为 0：d[is.na(d)] &lt;- 0 更改数据框或矩阵的列名：colnames(dt)[1] &lt;- &quot;cn&quot; 去除包含 NA 的行 / 去除某些列包含 NA 的行 / 去除头尾包含 NA 的行12345678dt[complete.cases(dt),]dt[complete.cases(dt[,5:6]),]# 如果一定要用 is.na()dt[rowSums(is.na(dt[,5:6]))==0,]# 去除头尾包含 NA 的行，中间行包含 NA 则不去除df[min(which(complete.cases(df)==1)):max(which(complete.cases(df)==1)) ,] Factor Factor 转换为 numeric as.numeric(as.character(factorname)) as.numeric(levels(factorname)[factorname]) Statistics set.seed()：是用于产生随机数的，编号设定基本可以随意 根据 t 值求 p 值：p.value = 2*pt(-abs(t.value), df=length(data)-1) 从模型中抽取 Log-Liklihood 值：logLik(mylogit) 抽取模型的拟合值：fitted() 创建两个分类的 Dummy：as.numeric(gear_box==&quot;手自一体&quot;) 计算四分位数间距：IQR() 计算百分位数：quantile(x, probs=c(0.25, 0.75)) Advanced FunctionReduce Reduce takes a binary function and a list of data items and successively applies the function to the list elements in a recursive fashion. For example:12Reduce(intersect,list(a,b,c))` is the same as `intersect((intersect(a,b),c)Reduce(function(x,y) merge(x,y,by="cust_no"), list(vege_loyal,fruit_loyal,meat_loyal)) Apply Family datacamp_tutorial apply(x, margin, fun)：x is an array or matrix, margin = 1 for row / 2 for column, such as apply(matrix, 2, sum) lapply：和 apply 的不同是也可以应用于 dataframes, lists or vectors，并且 return list lapply(MyList,&quot;[&quot;, , 2)：从列表的每一个矩阵中选取第二列元素，返回一个向量列表 The [ notation is the select operator The [[ ]] notation expresses the fact that the we are dealing with lists sapply：和 lapply 类似，但不同是 sapply 返回最基础的数据结构，而不是列表 sapply(MyList,&quot;[&quot;, 2, 1 ):选取列表每一个矩阵中的第二行第一列的元素，返回一个向量 unlist(lapply(MyList,&quot;[&quot;, 2, 1 )):lapply 的结果 unlist 之后才返回向量 rep(): sapply 返回结果配合 rep 函数使用，比如 rep(c(1,4,8),c(3,1,2)) 返回 c(1,1,1,4,4,8) mapply: applies a Function to Multiple List or multiple Vector Arguments, such as mapply(rep,1:4,4)，相当于 rep(c(1,2,3,4),c(4,4,4,4)) 或者 c(rep(1, 4), rep(2, 4), rep(3, 4), rep(4, 4)) sweep:比如矩阵每一列减去各列均值data &lt;- sweep(data, 2, data_means,&quot;-&quot;)，均值可以先通过 apply 求出 Eval &amp; Assign Eval() 函数可以将字符串转变为变量名 如果用字符串表示的变量无需被赋值，只是表示变量，则用 eval 函数转换：var &lt;- eval ( parse ( text = &quot;string&quot; ) ) Assign() 函数可以为以字符串命名的变量赋值 如果用字符串表示的变量是被赋值对象，则使用 assign 函数 assign(paste(str1,str2,sep=&quot;&quot;), value) DateTime Transfermation 将字符串转换为 Date 类型 as.Date(&#39;20140325&#39;,&quot;%Y%m%d&quot;) strptime(&#39;2012-09-16 19:35:58&#39;,&quot;%Y-%m-%d %H:%M:%S&quot;) lubridate 获取年份：year(as.Date(&#39;20130204&#39;,&quot;%Y%m%d&quot;)) 获取月份：month(as.Date(&#39;20130204&#39;,&quot;%Y%m%d&quot;)) 获取周数 从1月1日开始计算： week(as.Date(&#39;20130204&#39;,&quot;%Y%m%d&quot;)) 从星期一开始计算：isoweek() 获取该月第几天：mday(as.Date(&#39;20130204&#39;,&quot;%Y%m%d&quot;)) 获取该年第几天：yday(as.Date(&#39;20130204&#39;,&quot;%Y%m%d&quot;)) 将 Excel 中的数字型日期时间转换为日期 as.Date(41310.11, origin = &quot;1899-12-30&quot;) 41310 的单位是天 as.POSIXct(41310*24*3600, origin = &quot;1899-12-30&quot;) POSIXct 函数的单位是秒 求两个日期之间的差12difftime(as.Date(as.integer(41310.12), origin = "1899-12-30"), as.Date('20130204',"%Y%m%d"), units='days')as.integer(difftime(max(t1,t2,t3,t4), min(t1,t2,t3,t4), units="mins")) Time Series 获取时间序列的日期：as.Date(time(ts)) 获取时间序列的起止时间：start(ts) end(ts)，返回的都是 numeric 数组，通过下标获取年月 根据起始时间产生连续的时间seq.Date，然后创建 zoo 对象 123456from &lt;- as.Date(&quot;1974-01-01&quot;)to &lt;- as.Date(&quot;1989-12-31&quot;)months &lt;- seq.Date(from=from,to=to,by=&quot;month&quot;)values &lt;- rep.int(0,length(months))Zooserie &lt;- zoo(values, months) String Processing12345678910111213141516171819# 分割字符串strsplit('abc-ds','-')[[1]][2]# 判断一个字符串是否包含在另一个字符串中grepl('-','dadas-')# 统计长度length() # 对象 object 的长度nchar() # 字符串的长度# 获取一个字符串的子串substr(string, start_index, end_index)# 替换字符串中的字符gsub('e', '', 'e123') # 返回 123# 判断一个字符串的开头结尾是否为特定字符startsWith("2013-01", "2013-")endsWith("2013-11", "11") Set Operation a &lt;- c(&quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) b &lt;- c(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;) 交集：intersect(a,b) = d,e 并集：union(a,b) = b,c,d,e,f,g 差集：setdiff(a,b) = b,c / setdiff(b,a) = f,g R Packages require() 和 library() 最大的不同是前者返回一个 Logical Value，如果包不存在则继续运行1234if (!require("abc", character.only=T, quietly=T)) &#123; install.packages("abc") library("abc, character.only=T)&#125; Data Manipulation &amp; Input &amp; Output Excel File openxlsx: data &lt;- read.xlsx(&quot;abc.xlsx&quot;, sheet = 1, startRow = 2, colNames = TRUE) 只能读取 .xlsx XLConnect:wb = loadWorkbook(&quot;../data_survey/A1.xls&quot;) df = readWorksheet(wb, sheet = &quot;Sheet1&quot;, startRow = 2, header = FALSE) tidyr gather()：将除了 iteration 的所有列转变为 key-value 的形式：recall &lt;- gather(recall_i, key=&#39;keyname&#39;, value=&#39;valuename&#39;, -iteration) spread()：将 key:value 的形式重新转变为 column 的形式 spread(recall,key,value) seperate()：将一列通过分隔符分拆为多列 unite()：多列合并为一列 dplyr window()：比如截取时间序列 window(ts, start=c(1949, 1), end=c(1959,12)) Ranking functions:min_rank() percent_rank() dense_rank() lead() &amp; lag()：不同包的 lag 函数用法不同，必须指定 dplyr::lag(ts,1) sqldf: df &lt;- sqldf(&#39;select * from dataframe&#39;) texreg：Conversion of R Regression Output to LaTeX or HTML Tables stargazer：Well-Formatted Regression and Summary Statistics Tables formatR: Format R Code Automatically、Introduction gridExtra：grid.table() 将表格输出为图片 Big Data &amp; Optimization optim:How to use optim() in R Rcpp: High Performance Functions with Rcpp data.table Getting Started_Github Tutorial ， 中文教程 data.table 的计算语法规则：`DT[i, j, by] 使用 i 来 subset 行，然后计算 j ，最后用 by 分组 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 计算指定行的变量均值：test[GROUP_ID==1 &amp; CLASS_ID==100, .(m_qty=mean(QTY))]# 统计各个分组满足条件的行数, .N 是一个内建的变量，它表示当前的分组中对象的数目test[GROUP_ID==1, .N, by=group_id]# 按照分组变量排序并计算平均值flights[carrier == "AA", .(mean(arr_delay), mean(dep_delay)), keyby=.(origin, dest, month)]# keyby 默认按升序排序，如果想改变排序flights[carrier == "AA", .N, by=.(origin, dest)][order(origin, -dest)]# by 也可以指定表达式，比如计算起飞延误和到达延误的航班各有多少（生成4行）flights[, .N, .(dep_delay&gt;0, arr_delay&gt;0)]# 同时对多列进行计算,.SD 包含了除分组变量之外的所有列DT[, lapply(.SD, mean), by=ID]# 返回每个分组的前两行ans &lt;- flights[, head(.SD, 2), by=month]# 根据 ID 连接某个变量的所有字符串dt[, lapply(.SD, paste0, collapse=" "), by = ID]# .SDcols 可以指定 .SD 中包含哪些列，而不是除分组变量之外的所有列flights[carrier == "AA", lapply(.SD, mean), by=.(origin, dest, month), .SDcols=c("arr_delay", "dep_delay")]# 快速读取大文件fread('example.csv', sep=',', header=TRUE, integer64="character", encoding='UTF-8')# 数据选取：选取行flight[1:10]# 数据选取：选取列（返回 vector）flight[, arr_delay]# 数据选取：选取列（返回 data.table）,对 data.table 赋值时使用！flight[, list(arr_delay)]flights[, .(arr_delay, dep_delay)]flight[, "arr_delay"]# with=FALSE，myvector 才会被看做列名，并返回 data.table，否则返回向量mtcarsDT[, myVector, with=FALSE]# := operator 添加新列/移除一列/更改指定行某一列的值DT[,new_colname:=42]DT[,extant_colname:=NULL]trade[date=="2013-12-31", year_week := "2014-01"]# := operator 性能对比system.time(for (i in 1:1000) DF[i,1] &lt;- i) # Elapsed 591system.time(for (i in 1:1000) DT[i,V1:=i]) # Elapsed 1.158 (511 times faster) Time Series kernlab：Gaussian Process Regression bsts： Baayesian Structural Time Series tseries： Time Series Analysis and Computational Finance fUnitRoots: Trends and Unit Roots urca: Unit root and cointegration tests for time series data forecast： Forecasting Functions for Time Series and Linear Models Plot Examples Cross validation example 1234checkresiduals(model) # 模型残差正态性和纯随机性检验图gghistogram(ts) # 时间序列取值柱状图ggseasonplot(ts, polar=TRUE) # 时间序列季节性雷达图autoplot(temp_ts, series="Data") + autolayer(forecast, series="Forecast") + autolayer(fitted(model), series="Fitted") # 真实值+拟合值+预测值 （当然也可以通过数据框存储数据然后自己作图） WaveletComp：小波分析和时间序列重构，Wavelet analysis and reconstruction of time series, cross-wavelets and phase-difference (with filtering options), significance with simulation algorithms. Algorithm &amp; Modeling lmtest： Testing Linear Regression Models RStudioShortcut Key in RStudio Clear console: Ctrl + L Insert a chunk in Rmd: Ctrl + Alt + I Add comment: Ctrl + Shift + C R markdown HTML Documents knitr language engines 123456789101112output: html_document: toc: true toc_depth: 6 number_sections: false toc_float: collapsed: false smooth_scroll: false theme: readable highlight: tango code_folding: show df_print: paged Cheet Sheet 生成 HTML 时，如果找不到对象，先 load(file=&quot;.RData&quot;) 屏蔽代码提示和警告信息：{r message=FALSE, warning=FALSE} OthersRun R in CentOS 在 Centos 中安装 R 123sudo yum install -y epel-release sudo yum update -y sudo yum install -y R 运行 R 脚本并输出信息到终端: Rscript a.R 运行 R 脚本并输出到文件（默认 a.Rout）： R CMD BATCH a.R 123R CMD BATCH a.R# Check the outputcat a.Rout 断开终端后离线运行 R 脚本 安装 screen：yum install screen 新建一个会话：screen -S lnmp (lnmp为会话名，可自己定义) 离开会话并让程序继续运行：ctrl a d (按住ctrl不放，分别按 a 和 d) 恢复后台运行的会话：screen -r lnmp（lnmp为自己定义的会话名） 显示所有screen创建的会话：screen -ls 在会话里执行 exit 命令会话是结束运行并退到 shell 中 也可以使用 nohup 命令：nohup R CMD BATCH wenjian.R wenjian.out Project Management Get data out of R: 1 2 从另一个 R Project 中读取对象的方法： 先保存一个项目中的对象：saveRDS(df, file=&quot;mytweets.rds&quot;) 再到另一个项目中读取：df2 &lt;- readRDS(&quot;mytweets.rds&quot;) getwd(): 查看当前工作目录 ls(): 列出所有对象的名字 ls.str(): 列出所有对象的详细信息 save.image(): 保存工程！ Update R 所有包均安装在 R 安装路径的 Library 目录中，直接拷贝转移即可！ 当然，也可以通过下列命令恢复已安装的包 启动当前版本，输入以下命令 oldip &lt;- installed.packages()[,1] save(oldip, file=”installedPackages.Rdata”) 卸载旧版本 下载安装新版本，启动新版本输入以下命令 load(“installedPackages.Rdata”) newip &lt;- installed.packages()[,1] for(i in setdiff(oldip, newip)) install.packages(i)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning 算法学习心得和代码实现]]></title>
      <url>%2F2017%2F02%2F08%2FNavi1-Machine-Learning%2F</url>
      <content type="text"><![CDATA[持续更新：Machine Learning 和 Data Mining 的算法学习心得和代码实现 Awesome Website Machine Learning in R: mlr Basic ConceptCoefficient of Variation Also known as Relative Standard Deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. $CV=Standard \ Deviation/Mean$ Feature EngineeringFeature Selection Blog: A Summary of Feature Selection in Machine Learning Time Series PredictionGaussian Process Regression Blog: Time Series Prediction by Gaussian Process Regression Bayesian Structural Time Series Paper: Scott, S. L., &amp; Varian, H. R. (2014). Predicting the present with bayesian structural time series. International Journal of Mathematical Modelling and Numerical Optimisation, 5(1-2), 4-23. Rmd: A Tutorial for Bayesian Structural Time Series Model Evaluation and SeletionRegression Blog: Measures of Univariate Prediction Accuracy Classification Blog: Classification Model Evaluation and Seletion Text MiningTopic Modeling Blog: An Introduction to LDA and Other Topic Models]]></content>
    </entry>

    
  
  
</search>
